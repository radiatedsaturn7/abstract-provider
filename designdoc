Design Document for an Abstract Multi-Cloud Terraform Provider

Abstract Cloud Terraform Provider – Design Document
Introduction
The Abstract Cloud Provider is a custom Terraform provider that enables defining cloud-agnostic infrastructure resources which can be deployed across AWS, Azure, and GCP. Instead of writing separate Terraform configurations for each cloud, users can use generic resource types (e.g. Cluster, Bucket, Function, etc.) with a type field indicating the target cloud. The provider translates these abstract resources into the appropriate cloud-specific resources under the hood. By focusing on services common to all major providers, this abstraction follows a deliberate "lowest common denominator" approach
developer.hashicorp.com
 – avoiding cloud-specific features (for example, AWS Aurora or Azure Cosmos DB) in favor of broadly supported primitives. This design document outlines the provider's architecture, the schema of each resource, mappings to underlying cloud services, handling of multi-cloud deployments, and considerations for future extensibility. The goal is to provide a consistent user experience and configuration interface, while the provider takes on the complexity of provisioning the correct resources in each cloud. All implementation will be done in Go, as is standard for Terraform providers
hashicorp.com
, leveraging official cloud SDKs or APIs. The result will allow a single Terraform configuration to provision infrastructure on multiple clouds simultaneously (hybrid deployments), or to easily target a different cloud by changing a parameter, without rewriting the entire configuration.
Design Philosophy and Constraints
Cloud-Agnostic Abstraction: Terraform itself does not natively abstract across vendors, to avoid hiding unique features behind least-common interfaces
developer.hashicorp.com
. This provider intentionally creates a simplified abstraction, meaning we only include features that are present in all three clouds. This inevitably means sacrificing some advanced capabilities in exchange for portability. For example, we support a generic Database resource for basic relational databases, but not specialized offerings like Amazon Aurora or Spanner which have no equivalent in other clouds.
Lowest Common Denominator Features: Each abstract resource type encapsulates the minimal set of properties needed to provision a functional resource on any cloud. If a feature or setting cannot be mapped to all three providers in a reasonably equivalent way, it will not be a first-class field in the schema. Instead, such settings can sometimes be passed via the annotations map (see below) for provider-specific tweaks when absolutely necessary. This design avoids the complexity and cost of trying to emulate cloud-specific functionality on other platforms
discuss.hashicorp.com
.
Trade-offs and Portability: Users of the Abstract Cloud Provider must understand the trade-off: by using a single interface for multiple clouds, they gain consistency and reduce duplication, but they also accept a loss of cloud-specific optimizations. The design prioritizes portability and simplicity over maximizing each cloud’s capabilities. As noted in community discussions, fully abstracting even basic resources like VM instances can be challenging due to subtle differences
reddit.com
. Our approach mitigates this by focusing on higher-level constructs (e.g. managed clusters, serverless functions, object storage) where the cloud implementations are more analogous
discuss.hashicorp.com
.
Resource type Field: Each resource requires a type attribute to specify the target cloud provider (e.g. "aws", "azure", or "gcp"). This flag determines which cloud-specific API calls or Terraform configurations the provider will use. The type field is immutable after creation – changing it forces the destruction and re-provisioning of the resource in the new cloud (marked as ForceNew in schema), since Terraform cannot migrate an existing resource across providers.
annotations for Cloud-Specific Settings: Every resource schema includes an optional annotations map for passing through cloud-specific settings or flags that are not part of the core abstract schema. This allows flexibility when a particular cloud requires an extra parameter or the user wants to opt into a cloud-specific feature without breaking the overall abstraction. For example, an abstract_bucket might accept an annotation for an AWS-only setting like acl = "private" without introducing an acl field globally. The provider will interpret the annotations in the context of the specified type and ignore irrelevant keys. The use of annotations is meant as an escape hatch; the provider will document supported annotation keys for each cloud resource type. Importantly, reliance on too many annotations may reduce portability, so the design encourages using them sparingly – the common-case usage should not require annotations at all.
Hybrid Deployment Support: The provider is designed to handle configurations that include resources targeting different clouds in one Terraform run. For example, a single configuration could create an AWS S3 bucket, an Azure Function, and a GCP database together. Terraform will manage the dependency graph normally – an abstract resource can reference another abstract resource’s attributes regardless of cloud, since all resources are managed by the one provider. No special handling is needed beyond ensuring credentials for all target clouds are available. We do ensure that references between resources only use common output attributes (e.g. using a bucket’s name or URL in a function’s environment variable) rather than cloud-specific identifiers that wouldn’t make sense cross-cloud. The provider does not attempt to directly connect resources across clouds (for instance, attaching an AWS Lambda to an Azure storage trigger), but it makes it straightforward to compose such multi-cloud architectures by sharing data through Terraform outputs and variables. In essence, interpolation of values between clouds is fully supported (e.g., using an AWS-generated resource URL in an Azure resource configuration), but it’s up to the user to use those values meaningfully (such as coding a function to call an external service in another cloud).
Terraform Provider Implementation: The provider will be implemented in Go (using HashiCorp’s Terraform Plugin SDK v2 or the newer Terraform Plugin Framework). This is the standard and recommended practice for Terraform providers
hashicorp.com
, ensuring we can compile the provider to a single binary plugin. Each abstract resource corresponds to a resource implementation in the provider code with CRUD (Create, Read, Update, Delete) operations. Internally, those operations will invoke the respective cloud’s SDK/API calls based on the type. For example, the Bucket resource’s create function will have logic like: if type=="aws" call AWS S3 API to create a bucket; if type=="azure" call Azure Storage SDK to create a storage account and container; if type=="gcp" call GCP Cloud Storage API to create a bucket, and so on. The provider will manage multiple cloud API clients under the hood (one for AWS, one for Azure, one for GCP), initialized with credentials from the provider configuration. The state stored by Terraform will contain the necessary identifiers for each cloud’s resources (like ARNs, Azure resource IDs, etc.), but the user will mostly interact with high-level outputs (names, URLs, etc.). By structuring the code with clear separation for each cloud’s logic (for maintainability), we ensure adding a new cloud or updating an API is straightforward.
Provider Configuration (Credentials): To use this provider, the user must configure access to each cloud they intend to use. The provider supports authenticating to AWS, Azure, and GCP. This can be done via environment variables (e.g. AWS_ACCESS_KEY_ID, Azure client credentials, GCP credentials JSON, etc.) or via an abstract provider configuration block in Terraform. For instance, the provider config may allow specifying credentials for each cloud and default settings like default region:
provider "abstract" {
  aws = {
    region     = "us-west-2"
    access_key = var.aws_access_key
    secret_key = var.aws_secret_key
  }
  azure = {
    subscription_id = var.azure_sub_id
    client_id       = var.azure_client_id
    client_secret   = var.azure_client_secret
    tenant_id       = var.azure_tenant_id
    location        = "westus2"
  }
  gcp = {
    project    = var.gcp_project
    region     = "us-central1"
    credentials = file("gcp-key.json")
  }
}
The above shows a possible configuration interface: credentials and a default region/location per cloud. The provider will use these defaults when a resource’s region (or location) is not explicitly specified. Each resource can also override the region in its schema if needed (for example, to place one bucket in a different region). If a cloud’s credential block is omitted, the provider will attempt to find credentials from environment or well-known locations (e.g., ~/.aws/credentials or Google Application Default Credentials), similar to official providers.
Naming and Global Uniqueness: Where applicable, the provider will document naming requirements that differ by cloud. For example, S3 and GCS bucket names must be globally unique and DNS-compatible, whereas Azure storage container names must be unique within a storage account. The abstract resources will try to enforce the superset of these rules (e.g., allowed characters) to prevent user error. If a name is valid for one cloud but invalid for another, the provider may either automatically adjust it (with a predictable transformation) or produce a validation error if the type is known. The goal is to avoid runtime errors from clouds by catching configuration issues in Terraform plan/apply phase.
Resource Mapping Strategy: For each abstract resource, the provider might need to create multiple underlying cloud resources. The user perceives a single resource, but for example, an Azure Function requires not only the function itself but also a storage account and a hosting plan. The provider will manage these ancillary resources internally, either by creating them implicitly or by expecting certain annotations. The design leans toward simplicity: if a cloud’s service requires a supporting resource that has no equivalent concept in others, the provider will attempt to manage it automatically (to avoid burdening the user with cloud-specific setup). For instance, Azure requires a Resource Group for any resource – the provider can either use a default resource group name (e.g., provided in the provider config or a global default like "abstract-provider-rg") or let the user specify one via annotations.resource_group. In AWS, a default VPC might be used for networking if none is specified, whereas in Azure/GCP the provider might auto-create a VNet or use the cloud’s default network. These decisions are detailed per resource below. The guiding rule is: if a reasonable default or automatic step exists to bridge a cloud-specific requirement, the provider will implement it, so that the user’s experience remains as cloud-neutral as possible.
Supported Abstract Resources and Cloud Mappings
The Abstract Cloud Provider will support the following resource types initially, each corresponding to analogous services in AWS, Azure, and GCP:
Abstract Resource	AWS Equivalent	Azure Equivalent	GCP Equivalent
Network	VPC (Virtual Private Cloud)	Virtual Network (VNet)	VPC Network
Instance	EC2 Instance	Azure Virtual Machine (VM)	Compute Engine VM Instance
Cluster	EKS Kubernetes Cluster (plus node group)	AKS Kubernetes Service (managed cluster)	GKE Kubernetes Cluster
Bucket	S3 Bucket	Azure Blob Storage Container (in a Storage Account)	Cloud Storage Bucket
Function	Lambda Function	Azure Function (Function App + Function)	Cloud Function
Database	RDS Database Instance (MySQL/PostgreSQL)	Azure Database for PostgreSQL/MySQL (Single Server/Flexible Server)	Cloud SQL Instance (MySQL/PostgreSQL)
Each abstract resource provides a normalized interface to create the above services. Below we detail each resource’s schema and behavior, including how it maps to each cloud provider.
Abstract Network
Description: An abstract_network resource represents a virtual network for isolating resources and defining IP ranges. This maps to a VPC in AWS, a Virtual Network in Azure, and a VPC Network in GCP. All three clouds share the concept of a private network with subnets, although their defaults differ. The abstract provider uses this resource to allow users to create network infrastructure that other resources (like Instances or Clusters) can attach to, if they want control over IP ranges or segmentation. Cloud mappings:
AWS: Creates a new VPC. By default, it will also create a couple of default subnets (e.g., one per availability zone in the chosen region) and an internet gateway, mimicking AWS’s default VPC structure. If the user provides a cidr (CIDR block), that will be used for the VPC’s address space; otherwise a default like "10.0.0.0/16" may be assumed. The resulting VPC ID is tracked. Any Instance or Cluster of type AWS can reference this network by ID to launch instances in it (the provider will then place those instances in the created subnets). If no network is specified for an AWS resource, the AWS default VPC (if available in the account) could be used as a fallback.
Azure: Creates an Azure Virtual Network in a specified resource group. The cidr will define an address space for the VNet (default could be "10.0.0.0/16" if not provided). The provider will also create a default Subnet within that VNet (e.g., using the same CIDR or a /24 slice of it) since Azure VMs/AKS require at least one subnet. The Virtual Network’s ID and the default Subnet ID are recorded. If the user doesn’t specify a resource group via annotations.resource_group, the provider will use a global one from provider config or create one (e.g., "abstract-default-rg"). Other Azure resources can then be placed in this network/subnet if specified.
GCP: Creates a VPC Network (in auto-mode or custom-mode). By default, we might choose auto mode to automatically create subnets in each region (to mimic how GCP’s default network works). If a cidr is given, we create a custom-mode VPC with a single subnet covering that CIDR in the specified region (or if no region given, perhaps default region from provider config). The network’s self-link or ID is saved. GCP instances or GKE clusters can use this network if provided; otherwise, the default VPC in the project is used by default.
Schema Design: The abstract_network resource schema focuses on defining the network range and optional parameters:
name – string, optional: Name for the network. This might be used as the cloud network name/tag. If not provided, a default name can be generated. (Azure requires a name for VNet, AWS VPC can have a Name tag, GCP network needs a name.)
cidr – string, optional: The IP address range in CIDR notation for the network. Example: "10.50.0.0/16". If not set, the provider uses a sensible default (which could differ per cloud, e.g., AWS default VPC uses 172.31.0.0/16; we might just use 10.0.0.0/16 universally for new networks).
type – string, required: The target cloud ("aws", "azure", or "gcp").
annotations – map(string->string), optional: Cloud-specific settings.
For AWS, annotations might include keys like enable_dns_support or enable_dns_hostnames to control VPC settings (if not, defaults to true).
For Azure, could specify resource_group if the network should be created in a particular resource group, otherwise uses provider default.
For GCP, perhaps auto_create_subnetworks (true/false) to choose auto or custom mode (default to true for auto-mode if no CIDR given).
Output attributes: The network resource would export at least an id (which could be the VPC ID, VNet resource ID, or GCP network name/URI). It may also export subnet_id (the ID of a default subnet created, if applicable) and potentially cidr (normalized) or other properties like gateway_id (AWS internet gateway) if needed for routing.
Behavior and Constraints: Changing the cidr or type of a network after creation will force a new network (since resizing or migrating networks is not generally possible without recreation). The provider will ensure that for each cloud a minimal viable network exists after creation:
AWS: VPC + at least one subnet + internet gateway (so that instances can have internet connectivity by default; security rules would still default to closed).
Azure: VNet + at least one Subnet.
GCP: VPC + subnets (auto or custom as determined).
If users want more advanced network setups (multiple subnets, specific routing, peering, etc.), those are beyond this abstraction’s scope – they would either use cloud-specific Terraform resources or future enhancements of this provider. The abstract_network is just enough to support deploying the compute resources in an isolated network.
Abstract Instance
Description: An abstract_instance resource represents a single virtual machine (VM) instance with basic settings, mapping to an EC2 instance on AWS, an Azure VM, and a Google Compute Engine VM on GCP. This is a fundamental IaaS building block. The abstraction covers launching a VM with a certain machine size and operating system in a specified region. Cloud mappings:
AWS: Creates an EC2 instance. If an abstract_network (VPC) is provided via a network_id or similar field, the instance will be created in that VPC (and in a subnet of that VPC, possibly the first available subnet). Otherwise, it will launch in the default VPC of the AWS account for the region. The provider will choose an AMI if the user hasn’t specified one (for example, using a default Amazon Linux 2 or Ubuntu image in that region). It will also create a basic Security Group to allow the instance to communicate (by default, perhaps open to outbound and allow no inbound except maybe SSH if a key is provided). If the user supplies annotations.key_name (an existing SSH key name) or similar, it will associate that for SSH access. Otherwise, AWS instance will have no key (not accessible via SSH unless user uses AWS Session Manager).
Azure: Creates an Azure Virtual Machine in a given location (and resource group). Azure requires more supporting resources: a Virtual NIC (network interface) attached to a subnet, a disk, and potentially an NSG (network security group). The provider will handle these: if network_id is provided and corresponds to an abstract_network (VNet), it will create a NIC in that network’s first (or default) subnet. If no network given, it will use Azure’s default network (if one exists) or create a new VNet implicitly (with a default address space). It will also create a disk for the OS (with default size e.g. 30GB unless specified in annotations) and attach it. If no custom image is given, it will use a default image (e.g., the latest Ubuntu LTS image from Azure marketplace, or another common OS). Azure VMs typically need admin credentials; the provider might generate a random admin username/password or require the user to provide an admin_password or SSH key in annotations (if none provided, a random password could be set and output as a sensitive value). The NSG (firewall) will be configured to allow outbound traffic and no inbound by default (similar to AWS). In short, the provider will orchestrate the creation of all necessary pieces so that the user only sees one resource.
GCP: Creates a Compute Engine VM instance. If network_id is provided, the instance’s network interface will use that network (and default subnet in the specified region). If not, it uses the default VPC network of the project. The provider selects a base image if none specified – likely a Debian or Ubuntu image from GCP’s public images. A service account for the VM can be auto-created or the default compute service account is used by default. Firewall rules in GCP’s default network allow certain traffic like SSH by default; if using a custom network, the provider might also create a default firewall rule to allow outbound and maybe SSH inbound similar to other clouds (or instruct the user to handle firewall via separate resources if needed). The instance machine type is determined by the size or instance_type field (see schema below), mapping generic sizes to GCP machine families (e.g., "small" might map to e2-medium or similar). Boot disk is created automatically using the chosen image.
Schema Design: The abstract_instance schema provides common properties for a basic VM:
name – string, optional: Name of the instance. If not provided, a unique name will be generated. For AWS this sets the Name tag, for Azure this is the VM name, for GCP the instance name.
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location to deploy the instance. If omitted, the provider’s default region for that cloud is used. (Azure also needs a resource group – if not provided via annotation, the provider will use the default resource group from config or create one.)
image – string, optional: An OS image identifier or shorthand. This field is abstracted to avoid cloud-specific image IDs. The user can provide generic values like "ubuntu-20.04" or "windows-2019" and the provider will look up a corresponding image in the target cloud (e.g., latest Ubuntu 20.04 LTS AMI in AWS, or Azure marketplace image SKU, etc.). If image is not provided, a default (e.g., latest stable Ubuntu LTS Linux) is used. Alternatively, the user can specify cloud-native image details via annotations if needed (for example, an exact AMI ID for AWS).
size – string, optional: A generic machine size or tier (e.g., "small", "medium", "large", or numeric like "2vCPU-8GB"). The provider will map this to an actual instance type in each cloud. For instance, "small" might map to an AWS instance type like t3.small (2 vCPU, 2GB), an Azure VM size like Standard_B1ms, and a GCP machine type like e2-small. The mapping will be documented and aims to choose roughly equivalent sizes. If size is not given, a default small size is used (suitable for a micro test server). As an alternative, the user could bypass this by specifying an exact instance type in the annotations (e.g., instance_type = "m5.large" for AWS) if they want a specific class – but doing so ties the config to that cloud’s naming.
network_id – string, optional: An ID of an abstract_network resource to attach this instance to. If provided, the instance will be created in that network’s context (VPC/VNet). If omitted, uses the cloud’s default network environment (default VPC, etc.). On AWS, if a subnet ID is needed, the provider picks a subnet in the given VPC (if the abstract_network created subnets, use the first; or create one). On Azure, the provider uses the VNet’s default or first subnet. On GCP, if using a custom network, ensure a subnet exists in the region.
public_ip – bool, optional: Whether to assign a public IP to the instance. Default could be false for security unless explicitly true. If true, the provider will allocate or enable a public IP (Elastic IP on AWS associated with the instance, Azure will create a Public IP resource for the NIC, GCP attaches a public IP by default if network is default or if configured).
annotations – map(string->string), optional: Cloud-specific parameters. Examples:
AWS: instance_type (if user wants to directly specify), key_name for an existing SSH key, security_groups to attach custom security groups, etc.
Azure: resource_group (to specify RG), size (Azure VM size if wanting to override generic mapping), admin_username, admin_password or admin_ssh_key for login (if not provided, the provider may generate credentials).
GCP: machine_type (if overriding size mapping), subnet_name if a specific subnet in the network should be used, service_account or scopes for the VM’s service account permissions.
Output attributes: Each instance will output an id which is the cloud identifier (e.g., the EC2 Instance ID, the Azure VM resource ID, or the GCP instance self-link). It can also output private_ip and public_ip (if assigned) for the instance, which are useful for configuring other components. Possibly hostname or instance_name could be output as well. Sensitive outputs like an admin password (if auto-generated) would be marked sensitive and ideally provided via Terraform’s output mechanisms.
Behavior and Lifecycle: Stopping, starting, or reconfiguration of instances is limited. Changes to certain fields:
Changing image or size on an instance will likely replace the instance (since resizing or reimaging is complex cross-cloud). We will mark those fields as ForceNew for simplicity.
Minor changes like adding an annotation for a security group might be handled in-place if the cloud allows (e.g., update AWS security groups).
The provider will ensure that deletion of an abstract_instance also cleans up any associated resources it created (e.g., Azure NICs, disks, public IPs if they were auto-created, etc.) to avoid orphaned resources or leaks.
Example usage: (for illustration, not a separate section)
# Example: Launch an instance in each cloud using similar config
resource "abstract_instance" "aws_vm" {
  name    = "app-server"
  type    = "aws"
  size    = "small"
  image   = "ubuntu-20.04"
  network_id = abstract_network.vpc.id   # using an abstract network defined elsewhere
  public_ip  = true
  annotations = {
    key_name = "my-ssh-key"  # AWS-specific: use an existing key for SSH
  }
}
resource "abstract_instance" "azure_vm" {
  name    = "app-server"
  type    = "azure"
  size    = "small"
  image   = "ubuntu-20.04"
  network_id = abstract_network.vnet.id  # Azure VNet from abstract_network
  public_ip  = true
}
resource "abstract_instance" "gcp_vm" {
  name    = "app-server"
  type    = "gcp"
  size    = "small"
  image   = "ubuntu-20.04"
  # Using default network in GCP by omitting network_id
  public_ip  = true
}
The above would create three similar VMs on each cloud. In practice, you might not deploy all three at once, but it shows portability: aside from type, the config is nearly identical. Each will pick an appropriate instance type and image for that cloud.
Abstract Cluster
Description: An abstract_cluster resource represents a computing cluster or orchestrated cluster service. We define it as a managed Kubernetes cluster in this abstraction, since all three cloud providers offer Kubernetes as a common service (Amazon EKS, Azure AKS, Google GKE). By using Kubernetes as the abstraction for "cluster", we ensure the consumer gets a consistent interface (a Kubernetes cluster API) on any cloud
discuss.hashicorp.com
. The cluster includes the control plane managed by the cloud and a set of worker nodes. This resource essentially provisions a fully managed K8s cluster on the chosen provider with a specified number of worker nodes and basic settings. (Rationale: We choose Kubernetes because it’s a standardized cross-cloud interface for running containerized workloads. Alternatively, a simpler “cluster of VMs” could be considered, but that is effectively covered by using multiple Instance resources or an auto-scaling group, which is less standardized across clouds. Kubernetes offers a higher-level, uniform target for applications, aligning with the provider-agnostic goal.) Cloud mappings:
AWS (EKS): Creates an Amazon EKS cluster. This involves two main components: the EKS control plane (managed by AWS) and worker nodes (which can be managed node groups or self-managed autoscaling groups). The provider will handle creating the EKS cluster itself (specifying Kubernetes version if provided, otherwise default) and a default managed node group. The user can specify node_count and a generic node_size, which the provider maps to an EC2 instance type for the nodes (e.g., "medium" -> t3.medium or similar). It will create that many nodes (or an autoscaling group of that capacity). AWS EKS requires an IAM role for the cluster and node group; the provider will create these roles/policies under the hood (unless the user supplies them via annotations). It also requires subnets to deploy the cluster into – if the cluster network_id is provided (referring to an abstract_network/VPC), the cluster will use subnets from that VPC. If no network is provided, the provider may create a new VPC and subnets dedicated to the EKS cluster (or use the default VPC if available, though best practice is usually a dedicated VPC). The output of an EKS cluster will include the Kubernetes API endpoint URL and an authentication token or kubeconfig data so that the user can actually use kubectl with the cluster. The provider will retrieve the kubeconfig (certificate, endpoint, etc.) and make it available (possibly via an output attribute kube_config or by writing to a local file if configured). By default, AWS’s EKS control plane is highly available across AZs; the user can optionally specify annotations.region_zones to control which AZs or how many, but that might be beyond common denominator and thus not exposed directly.
Azure (AKS): Creates an Azure Kubernetes Service cluster. Similar to EKS, Azure’s managed Kubernetes needs a resource group, and optionally can integrate with other Azure resources. The provider will create an AKS managed cluster with default settings. It will ensure a node pool of node_count nodes using a VM size mapped from node_size. For example, "medium" might correspond to Standard_DS2_v2 instances for AKS nodes. Azure requires a few ancillary things: a service principal or managed identity for the AKS, and a Virtual Network. The provider can create a system-assigned managed identity for the AKS (Azure handles that if not provided) and will create or use a Virtual Network. If network_id is given (pointing to an abstract_network), the AKS cluster will be created in that VNet (and its nodes in a subnet). If not, the provider can either create a new VNet/subnet or use the Azure default infrastructure (Azure doesn’t automatically have a “default VNet” per se, so likely it will create one). The output for AKS will include the Kubernetes API server endpoint and credentials. Terraform’s AKS resource normally can output a kubeconfig; our provider will do similarly, outputting kube_config (or components like host, client certificate, token).
GCP (GKE): Creates a Google Kubernetes Engine cluster. This is straightforward as GKE is fully managed by Google. The provider will request a GKE cluster with the given number of nodes (node_count) and a machine type mapped from node_size (e.g., "medium" -> e2-medium nodes or similar). If network_id is provided, the cluster will be created in that VPC network (and in a new subnet or existing one in the specified region). If no network is specified, it uses the default VPC. GKE typically will create a default node pool if not specified; we will explicitly create one with the desired count and size. GKE outputs a cluster endpoint and an auto-generated kubeconfig as well (or one can be fetched with gcloud). The provider will capture the cluster’s endpoint, CA certificate, and generate an access token or use application default credentials to allow the user access. These will be provided as output attributes, similar to AWS and Azure.
Schema Design: The abstract_cluster focuses on high-level cluster settings:
name – string, optional: Name of the cluster. If omitted, a default name is generated. This will be used to name the cluster in the cloud (EKS cluster name, AKS resource name, GKE name).
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location for the cluster. If not set, uses provider default region for that cloud. (Note: Azure AKS requires a region and a resource group; the region can be deduced from resource group’s location if RG given. The provider can create a new RG in the specified region if needed.)
kubernetes_version – string, optional: Kubernetes version for the cluster (e.g., "1.26"). If not provided, the cloud will default to a current stable version. This field is common to all three (each lets you choose a version).
node_count – number, optional: Number of worker nodes in the cluster’s default node pool. Default might be 3 (a typical small cluster) if not specified. The provider will create this many nodes. If the cloud auto-scales or requires a minimum, we can set auto-scaling parameters via annotations (e.g., min/max size).
node_size – string, optional: Generic size for the worker nodes (similar to instance size). For example "medium" or "large". The provider maps this to an instance type in each cloud’s context (like it does for instances). If not provided, a default (e.g., medium) is used.
network_id – string, optional: Reference to an abstract_network if the cluster should be launched in a specific network/VPC. Highly recommended for AWS and Azure to control network placement. If not provided, the provider will implicitly handle networking: e.g., create a new VPC or VNet just for this cluster (with appropriate subnets and routing). On GCP, if not provided, uses default network.
annotations – map(string->string), optional: Cloud-specific optional settings:
AWS: could accept eks_role_arn if the user wants to supply an existing IAM role for the EKS cluster, or eks_node_role_arn for node IAM role (otherwise provider creates roles). Could also allow node_instance_type if user wants a specific instance beyond what node_size chooses. Another useful annotation might be fargate = true to create an EKS Fargate (serverless) profile instead of nodes, if that were in scope (but likely not for initial design since Azure/GCP analog would be AKS virtual nodes or GCP autopilot – those features might be skipped initially).
Azure: resource_group to specify an existing RG to create the AKS in. node_vm_size if specific. Possibly enable_auto_scaling, min_count, max_count if user wants cluster autoscaler (since Azure and GCP support it; AWS EKS node groups too). We might keep auto-scaling out of core and add later, but it's something common enough that could be added in future.
GCP: project (if wanting to override default project), enable_autopilot (if one wanted GKE Autopilot mode rather than standard, though that changes semantics of node_count), etc., or node_machine_type.
Output attributes: All clusters will output:
id: an identifier for the cluster (could be the ARN for EKS, resource ID for AKS, name or self-link for GKE).
kube_config: A sensitive output containing the kubeconfig credentials to connect to the cluster. This might be a raw kubeconfig file content or a map of connection data (host, CA cert, token). For example, abstract_cluster.my.id might not be very useful by itself, but abstract_cluster.my.kube_config could be used by automation to configure kubectl or deploy apps. We ensure this output is marked sensitive in Terraform state (since it contains credentials).
Possibly endpoint: the Kubernetes API server URL (for informational use), and cluster_ca_certificate or client_certificate depending on auth (some clouds give client certs, some use token).
node_count: (for awareness, the actual number created, which should match input unless autoscaled).
We might output node_ids or node_instance_ids (like EC2 instance IDs of workers, node pool names, etc., mostly for debugging or if needed to attach other resources). However, that introduces cloud-specific detail, so we might not expose node IDs in the generic interface.
Behavior and Notes: The provider will manage cluster creation and handle differences:
EKS can take ~10+ minutes to provision; the provider must poll until the cluster is ready and nodes are registered. Similarly, AKS and GKE creation is asynchronous. The Terraform apply will wait until the cluster is fully ready (or at least until credentials can be fetched).
Credentials handling: AWS EKS typically requires invoking AWS STS to get a token for kubeconfig (unless using the AWS IAM authenticator). We might leverage the AWS Terraform provider logic or AWS SDK to get a short-lived auth token, or create a Kubernetes config with exec authentication (which might complicate usage). Alternatively, we could create a Kubernetes service account in the cluster with admin and fetch its token – but that’s additional steps. For simplicity, initial version might just output instructions to use the AWS CLI to get credentials, but that’s not user-friendly. Ideally, we output a ready-to-use kubeconfig. Azure AKS and GCP GKE offer direct client certificate and kubeconfig data, so easier there. We will aim to provide a unified kubeconfig output for all.
Upgrades and changes: Changing node_count can be applied in place for AKS and GKE (and for EKS managed node groups) – we will attempt to update the node pool size if possible without recreating the cluster. Changing kubernetes_version might trigger an upgrade process (if supported by the cloud) or force recreation if not easily upgradable. Possibly mark it ForceNew to avoid complexities of orchestrating upgrades.
If a user wants to destroy an abstract_cluster, the provider will delete the cluster and all nodes. For AWS, that includes deleting the node group and possibly the CloudFormation stacks EKS uses behind scenes (the provider will call the appropriate APIs to delete cluster and node group). Any implicitly created VPC for the cluster should also be deleted if not used elsewhere (though Terraform won’t know dependencies outside it; we might choose to not auto-create a VPC and instead always require network or default, to avoid tricky deletion scenarios).
Multi-cloud considerations: Two cluster resources of different types (clouds) have no direct relationship, but a user could use them in a config if they want clusters in multiple clouds for redundancy. Our provider handles that concurrently.
Abstract Bucket
Description: An abstract_bucket provides cloud-agnostic object storage. This corresponds to an S3 bucket on AWS, a Blob Storage container on Azure, and a Cloud Storage bucket on GCP. Object storage is a key common service for files and blob data, and this abstraction ensures a uniform way to create buckets and configure basic options like versioning. Cloud mappings:
AWS (S3): Creates an Amazon S3 bucket with the specified name. S3 bucket names are globally unique, so the provider will directly use the name given. The region can be specified (S3 allows choosing a region for bucket data; if not provided, defaults to AWS provider region). Common S3 features like versioning or server-side encryption can be toggled – versioning is exposed as a top-level field since all providers support it. If versioning is enabled, the provider will call to enable bucket versioning. Other S3 settings like ACL (public/private) can be controlled by annotations (acl key). By default, the bucket will be private (no public read access) unless annotations like acl = "public-read" are set, in which case the provider will apply that ACL. The provider may also consider annotations.force_destroy to allow emptying and deleting a non-empty bucket on destroy (to mimic Terraform’s force_destroy parameter for S3/GCS). If not specified, the user is expected to clean up objects or accept that destroy might fail on non-empty buckets.
Azure (Blob Storage): In Azure, object storage is a bit different – blobs reside in containers within a Storage Account. To fit this model, the abstract_bucket will create an Azure Storage Account (if needed) and a Blob Container. We have a few design choices:
We can create one storage account per abstract_bucket resource, using the bucket’s name (or a derived name) as part of the account name. Storage account names have stricter rules (lowercase, 3-24 chars, unique globally). If the given bucket name is valid and <=24 chars, we might use it directly as the account name; if not, we can generate one (like take first 20 chars + random suffix). The container inside will also have a name (which could reuse the same name if it meets container naming rules, which are similar to bucket rules).
Alternatively, we could allow an annotation storage_account to either use an existing storage account or to name the one to create. If a user wants multiple containers in one account (for cost or management reasons), they might create one abstract_bucket as the account and others as containers in it – but that breaks the one-to-one abstraction. Simplicity suggests each abstract_bucket corresponds to one container and its own storage account, isolating them (at the cost of potentially more accounts). We'll choose the simpler route: each Azure bucket results in a new Storage Account + Container, unless the user explicitly wants to reuse an account (then they can provide the storage_account name in annotations and the provider will create only the container in that existing account).
Region: Azure storage accounts are created in a region (Azure calls it location). We use the region field for this.
Other settings: If versioning is enabled, Azure accomplishes that via "soft delete" for blob versions (Azure supports blob versioning as of 2020). The provider will enable blob versioning on the storage account if requested. For encryption, Azure storage accounts are encrypted by default. Public access: Azure by default prevents public blob access at the account level; if an annotation like public_access = true is given, the provider will enable container-level anonymous read access (container ACL "blob" or "container").
Azure requires a resource group for the storage account. The provider uses annotations.resource_group if provided; otherwise, it uses the provider’s default RG or creates one (similar to earlier patterns).
GCP (Cloud Storage): Creates a GCP Cloud Storage bucket. Bucket names in GCP are globally unique like AWS. The region can be multi-region (like "US") or single region (like "us-central1"); if region is provided, we create a regional bucket, otherwise maybe a multi-region default (like "US" multi-region). The provider will call the Storage API to create the bucket with given name and location. Versioning is supported and will be enabled if requested. Public access can be toggled via annotations (e.g., public = true could remove all IAM restrictions or add a "allUsers: objectViewer" permission). Encryption can also be handled via annotation if needed (e.g., use Google-managed encryption vs customer-managed key). By default, GCP buckets block public access unless configured.
Schema Design: The abstract_bucket schema is straightforward:
name – string, required: The name of the bucket. This must conform to DNS-style naming rules (lowercase, no spaces, etc.) because AWS and GCP require it. Ideally 3-63 characters. The provider will validate the name format. (If a name is not unique or invalid for a particular cloud, the error will come from the cloud API; we surface that to the user).
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location for the bucket’s data storage. If omitted, a provider-specific default is used (e.g., for AWS, default region; for Azure, the default location from provider config; for GCP, a multi-region like "US" if not specified).
versioning – bool, optional: Enable versioning for the bucket. Default false. If true, the provider will turn on versioning (or the equivalent feature) for the bucket on all clouds.
annotations – map(string->string), optional: Cloud-specific extras:
AWS: acl (e.g., "private", "public-read"), force_destroy ("true"/"false" as string) to delete non-empty bucket on destroy, encryption (like "AES256" for S3 default encryption), tags for tagging.
Azure: resource_group to place the storage account, account_sku (e.g., "Standard_LRS" vs "Standard_GRS" for redundancy), public_access (e.g., "blob" to enable public read, or "none" to explicitly block public access – Azure uses a container-level setting).
GCP: storage_class (e.g., "STANDARD", "NEARLINE"), public (true/false to set allUsers read access), force_destroy (similar semantics to AWS, to auto-delete objects on bucket deletion).
Output attributes:
id: The bucket ID (for AWS/GCP this might just be the bucket name, since that uniquely identifies it; for Azure it could be the full resource ID of the storage account or a composite like <account>/<container>).
url: A URL or endpoint for the bucket. For example, an S3 bucket’s URL (e.g., https://my-bucket.s3.amazonaws.com), Azure blob container URL (https://<account>.blob.core.windows.net/<container>), or GCP bucket public URL (gs://bucketname or https://storage.googleapis.com/bucketname). Providing a URL helps users construct links or integrate across clouds (e.g., an Azure function might need the S3 bucket’s URL to fetch an object).
Possibly arn for AWS (S3 ARN), but that’s AWS-specific and usually not needed outside AWS. We likely omit cloud-specific identifiers from outputs unless necessary for interpolation with other resources. Instead, id can serve as an identifier to pass to other abstract resources (though currently none of the other abstract resources directly consume a bucket id, except maybe a function’s code source which we are not doing here).
Behavior:
Bucket creation is generally quick and synchronous on all clouds. The provider will verify creation and enable requested settings (versioning, ACLs).
Deletion: We consider force_destroy annotation to let the provider automatically delete all objects if the user wants Terraform to fully clean up. Otherwise, a bucket with objects will error on deletion (consistent with Terraform AWS/GCP provider unless force_destroy is true). We will document this to the user.
Immutability: Changing the name of a bucket or type will force new resource (you can't rename buckets in AWS/GCP easily, nor switch cloud obviously). Changing region would also recreate the bucket (data won't migrate).
Changing other settings like turning on versioning can be done in-place (the provider just enables versioning on the existing bucket).
The provider should handle Azure’s two-resource scenario carefully: when destroying an Azure bucket resource, it should delete the container and the storage account if it was created solely for this bucket. If multiple abstract_bucket resources share one storage account (via annotations), we must avoid deleting the account until the last container is gone. To simplify, we might discourage sharing and treat each abstract_bucket as owning its storage account unless explicitly told otherwise.
Abstract Function
Description: An abstract_function represents a serverless function deployed on a cloud’s Function-as-a-Service platform. This maps to AWS Lambda, Azure Functions, and Google Cloud Functions. It allows users to deploy code (typically small units of logic) without managing servers, in a cloud-agnostic way. The abstraction covers the function code, runtime, memory, and a basic trigger configuration. Cloud mappings:
AWS (Lambda): Creates an AWS Lambda function. The provider will package the given code (if it's provided as a local directory or file) into a ZIP if necessary and upload it to Lambda (either directly via AWS API or first to an S3 bucket if required for larger packages – the provider can handle small packages directly and fall back to S3 for larger). The runtime (e.g., Python 3.9, Node.js 14, etc.) is specified in a cloud-neutral way and mapped to AWS’s runtime identifiers (for example, "python3.9" maps to AWS runtime "python3.9", "nodejs14" -> "nodejs14.x" in AWS notation). The handler (entry point) needs to be specified for AWS (like "handler.function"). We might derive a default if not provided, or require it if the language needs it. Memory size and timeout are applied straightforwardly to Lambda. AWS Lambda also needs an IAM role for execution. The provider will create a default IAM role (with basic CloudWatch Logs permission) for the function unless one is provided via annotations (e.g., role_arn). If the user wants the function to be triggered by HTTP, AWS has Lambda Function URLs (a built-in HTTP endpoint) which can be enabled via an annotation or field trigger_http = true. If enabled, the provider will configure a Function URL and output the URL. (Alternatively, the user could connect an API Gateway, but that’s out of scope for this abstraction). We will expose the function’s ARN as id (or as a separate output), and possibly the function URL if enabled. Environment variables are supported and will be set as part of Lambda configuration.
Azure (Functions): Azure Functions are part of a Function App, which is essentially a container for one or more functions sharing resources. The provider will model each abstract_function as a separate Function App with a single function deployment, for simplicity. Specifically, for an Azure function, the provider will:
Ensure an Azure Storage Account exists (Azure Functions use it for handling triggers and logging). The provider can create a storage account per Function App (naming it after the function perhaps), or use a common one. Simplest approach: each function gets its own storage account (similar to how we did for buckets, to avoid name collisions and cross-usage complexities).
Create a Function App (in Consumption plan by default) in a given region and resource group. If annotations.resource_group is provided, use that; otherwise, use default RG or create one.
Deploy the code: if code is provided as a zip, we upload it to the function. If it’s source files, we may need to zip it first. Azure requires specifying a runtime stack for the Function App (like Node 14, Python 3.9, .NET, etc.), which corresponds to our runtime. The provider will map runtime strings to Azure Functions runtime settings (for example, "python3.9" -> use Python 3.9 runtime stack, "nodejs14" -> Node 14, etc.).
Azure Functions also require a name for the function and the Function App; we’ll use the name for both (with some normalization since function app name must be globally unique and <= 60 chars). The function’s entry point (function name) might be determined by code or a setting; we can require a handler field for consistency, but Azure often uses an index.js or run.ps1, etc. Possibly we just assume a convention or let the code package include the function definition as Azure expects (like a function.json). To keep it simple, we might require the user to provide a ready-to-run Azure Function package (with correct structure) if using Azure. This is an area where true abstraction is hard – runtimes and packaging differ. However, if our aim is to feed this to Codex, we might outline a simplified model: assume the code is a single function in a single file that each cloud can handle similarly (not fully realistic, but for design).
If trigger_http = true (if we choose to support triggers as a field), we will configure an HTTP trigger for the Azure function. By default, Azure Functions will treat a function named "HttpTrigger" with proper attributes as HTTP. We might instead ask the user to specify trigger type as part of configuration: e.g., trigger = "http" or trigger = "event". For now, support only "http" or none, since HTTP trigger is common to all. If HTTP trigger, the provider needs to set the function’s auth level to "anonymous" or "function" depending on if we want a public URL. We'll choose to allow public (anonymous) HTTP triggers if requested, and output the URL (which is typically https://<functionapp>.azurewebsites.net/api/<functionName>).
Environment variables are set via Application Settings in Azure Function App.
The provider will automatically create the needed Storage Account and Application Insights (if required – though App Insights is optional; Azure will create one by default for a new Function App unless disabled via an setting, we might disable it via annotation to reduce extraneous resources).
GCP (Cloud Functions): Creates a Google Cloud Function. GCP’s functions are quite straightforward: provide code (zip or source), specify runtime (e.g., "python39", "nodejs14"), entry point (function name), and trigger type. We will use Cloud Functions 1st Gen or 2nd Gen accordingly, but keep it simple. The provider will upload the code (the GCP API might allow direct upload or via Google Cloud Storage, which we handle internally possibly by using the GCP Go SDK to upload the zip). For runtime mapping, "python3.9" -> "python39", "nodejs14" -> "nodejs14", etc. Memory and timeout are set as specified. If trigger_http = true, we create an HTTP-triggered function (GCP supports that easily, just a flag). If no trigger specified (or if we wanted to support background triggers, that gets complicated, so default to HTTP or none), we can default to an HTTP trigger as it’s common. The function’s URL (for HTTP trigger) will be output (GCP gives a https endpoint). If not HTTP, the user could trigger it manually or via other GCP events (but those are cloud-specific; we likely only do HTTP for initial design).
GCP requires a project (from provider config) and region for the function. We use the region field for that (GCP Cloud Functions are regional).
The provider might need to enable the Cloud Functions API on the project if not already (which Terraform can’t directly do except via another resource; maybe assume it’s enabled or instruct user to enable beforehand).
Schema Design: The abstract_function schema centers on deploying code with config:
name – string, required: Name of the function. Will be used to name the function on each cloud (must meet naming rules: e.g., AWS allows [a-zA-Z0-9-_]+, Azure function app name must be unique DNS name, GCP function name has length and char limits). The provider may adjust or validate names accordingly (e.g., force lowercase for Azure DNS name).
type – string, required: "aws", "azure", "gcp".
region – string, optional: Region to deploy the function (AWS Lambda region, Azure Function region, GCP region). If omitted, uses default region from provider config for that cloud.
runtime – string, required: The runtime/language for the function. Examples: "nodejs14", "python3.9", "dotnet6", "go1.18". We will support a defined list of runtimes that all three clouds have some support for (Node.js, Python, perhaps Java). The provider will map this value to the exact identifiers:
AWS: directly corresponds in many cases (e.g., "python3.9" is an AWS Lambda runtime name).
Azure: we have to map to runtime stack settings (for example, Node 14 -> use NODE|14 setting in Application Settings, Python 3.9 -> PYTHON|3.9).
GCP: map to names like "nodejs14", "python39".
code – string, required: Path to the function code. This could be a local file or directory path. The provider will interpret it as follows:
If it’s a directory, it will zip the contents of that directory.
If it’s a zip file path, it will use it directly.
(In future, we might allow referring to an artifact in cloud storage, but for simplicity we assume local code packaged with Terraform.)
This is similar to how Terraform AWS lambda_function resource accepts filename. The provider then handles upload to the cloud.
handler – string, optional: The entry point of the function code. For interpreted languages, this is typically "file.function_name". For example, "index.handler" for Node or "main.my_func" for Python. AWS requires this field. Azure and GCP also need an entry point:
For Azure, if using a custom handler, we might require the function name (which would match a function in code or a configuration in function.json). This is tricky to abstract; but we could say if not provided, we assume the code package itself defines the function (Azure can detect a function via function.json).
For GCP, the handler is the name of the function in the code that will be invoked (e.g., HelloWorld).
We might make handler required for AWS/GCP and ignore it for Azure if Azure’s package structure self-describes the function (or also use it to generate a function stub).
memory – number, optional: Memory in MB for the function. Defaults to a baseline (e.g., 128 MB). All three providers allow setting memory. (Azure Functions on Consumption plan allocate resources differently, but one can set a memory limit in premium plans – however, we can assume 128MB triggers some default. Azure consumption plan might not allow manual memory setting; in that case, we may ignore or document that Azure will auto-scale the memory up to some limit).
timeout – number, optional: Timeout in seconds for function execution. Defaults to, say, 30 seconds. AWS supports up to 15 minutes, Azure default is 5 minutes (can be increased), GCP default 60s (max 540s for 1st gen). We'll allow up to the lowest common maximum, or just trust user to keep it reasonable. Setting it will apply in each (we’ll clamp to cloud max if exceeded).
environment – map(string->string), optional: Environment variables to set for the function. Each will be passed to the function at runtime. All three clouds support environment variables for functions. (We must be careful to store them as sensitive if they contain secrets – Terraform allows marking sensitive, which we should do if any value looks like a secret, but more simply we might treat all env as non-sensitive in state unless the user uses a sensitive var).
trigger_http – bool, optional: (Tentative common field) If true, the function will be deployed with an HTTP trigger/public endpoint. Default false. When enabled:
AWS: enable Lambda Function URL (which gives a URL like https://<id>.lambda-url.<region>.on.aws).
Azure: configure the function with an HTTP trigger (anonymous auth), resulting in a URL for the function.
GCP: deploy as an HTTP-triggered function (which automatically creates an HTTPS endpoint).
If false, the function is deployed with no public trigger (it can still be invoked by other cloud-specific means or manually). Security: The HTTP endpoints will be publicly accessible unless additional auth is required (not in scope for our abstraction – we assume if you set trigger_http, you want a public endpoint for simplicity).
annotations – map(string->string), optional: Cloud-specific options:
AWS: role_arn if using an existing IAM role for the Lambda (if not provided, provider creates a basic execution role), runtime_version (for finer control if needed), subnet_ids and security_group_ids if the Lambda should be in a VPC (not in all clouds, but if one wants Lambda in VPC to access DB, etc., they could specify those).
Azure: resource_group (for the Function App), plan_sku if wanting to specify a different hosting plan (Consumption vs Premium), disable_app_insights to not create Application Insights.
GCP: project (override), allow_unauthenticated (true/false, for HTTP trigger whether it’s open to public; by default we’d set true if trigger_http true, but user could require auth), vpc_connector if function should access a VPC.
Also, if in future we support non-HTTP triggers, annotations might carry things like event trigger specification (but out of initial scope).
Output attributes:
id: The function’s identifier (could be the ARN for Lambda, the Function App name or combined ID for Azure, and the name or full path for GCP).
invoke_url: If trigger_http was true, this will be the publicly accessible URL to invoke the function (for AWS, the Lambda function URL; for Azure, the function’s URL; for GCP, the HTTPS endpoint). If trigger_http is false, this may be empty or not present.
Perhaps invoke_arn for AWS if needed, but usually not required externally.
Possibly outputs to indicate the runtime or memory as deployed (mostly echoing inputs).
We will ensure not to expose sensitive info. If a function needs an API key (Azure functions have function keys for non-anonymous triggers), those would be output only if relevant and marked sensitive.
Behavior:
Code Packaging: The provider will likely need to package and upload code. This can be done using Go libraries to zip files and then using each cloud’s API. We must document that the code path should point to the correct content. Large packages might require special handling: e.g., AWS Lambda if >50MB requires S3 upload – our provider could transparently use an AWS S3 bucket (maybe the abstract_bucket resource or a temp bucket) to store the code and then point Lambda to it. But that’s an edge case; initially, we can set a reasonable limit and throw an error if exceeded.
Deploy & update: If the code changes (by checksum), Terraform will update the function in place (all providers support updating code without recreating the function). If certain configs change like runtime, some providers might require recreation or may support in-place update. We will attempt in-place updates for code, env, memory, timeout. Changing name or type obviously recreates on a different cloud. Possibly changing trigger_http from false to true can be done in place (enable a trigger) – we’ll handle enabling/disabling accordingly.
Concurrency and scaling settings (like AWS Lambda concurrency limits, GCP min/max instances, Azure plan capacity) are not exposed in the first version, as they are advanced. The defaults (auto-scale on demand) will suffice.
Dependencies: If a function needs to interact with other resources, it can reference them through Terraform as usual. For example, a function might use a bucket name from an abstract_bucket (as environment variable) to know where to fetch/store data. This is fully supported since the provider yields those values. Another case: if a function should be in the same network as an abstract_database (to access it without public internet), that’s tricky in AWS (Lambda in VPC) or Azure (Function in VNet). We may allow the user to provide VPC info via annotations for AWS and Azure (e.g., subnet_ids for Lambda, or use Azure Functions Premium Plan with VNet integration). But since network integration for serverless is a bit advanced, we can leave it for future extension. Initially, functions can be assumed to run with public access to the internet (thus they can reach any public endpoint including cross-cloud endpoints if necessary).
Abstract Database
Description: An abstract_database resource provides a managed relational database instance. It corresponds to AWS RDS (specifically a DB instance for a chosen engine), Azure Database for MySQL/PostgreSQL (or Azure SQL), and Google Cloud SQL. This abstraction lets users set up a basic SQL database (e.g., MySQL or PostgreSQL) in a cloud-agnostic way, covering the common parameters like engine type, version, instance size, and credentials. Cloud mappings:
AWS (RDS): Creates an RDS DB instance. We will support open-source engines that exist in all clouds: MySQL and PostgreSQL (possibly also MariaDB as it's in RDS and Azure, but GCP dropped MariaDB support in Cloud SQL? Actually GCP supports MySQL, Postgres, and SQL Server. MariaDB not sure. For simplicity, stick to MySQL and PostgreSQL). The user provides engine and optionally version. The provider will choose a DB instance class based on a generic size (like we do for VMs). If not specified, default to a small instance type (db.t3.small or similar). Storage size default perhaps 20 GB. Multi-AZ can be toggled via an annotation multi_az = true if desired (common enough to perhaps include as a top-level field, but since Azure Single Server vs Flexible with HA differ, we keep it as annotation). The DB will be created in a subnet group – if an abstract_network is provided (via a network_id field or similar), the provider will ensure the RDS is in that VPC. If network not given, it will use default VPC subnets. By default, the DB is not publicly accessible (we can default to private for security; if user wants public, an annotation like public = true can override to create with PubliclyAccessible true and set up security group to allow some ingress). The provider will set a master username and password. We may require the user to provide a password (to avoid storing it in state plain). Possibly accept password = var.db_password (marked sensitive) in the config. If not provided, generate a random password and output it as a sensitive value (the user then can retrieve it via Terraform output). The endpoint (hostname and port) will be output for connectivity. RDS also requires minor things like parameter groups; the provider will use the default parameter group unless advanced tuning is needed (out of scope).
Azure (Database for PostgreSQL/MySQL): Azure has two offerings: the older Single Server (which was for MySQL/Postgres) and the newer Flexible Server, as well as Azure SQL for MS SQL. Since we want cross-cloud, focusing on MySQL/Postgres, we can choose Flexible Server (which is the current recommended for MySQL/Postgres on Azure). The provider will create an Azure Database for MySQL or PostgreSQL flexible server in the given region. It also needs a resource group and a Virtual Network if using VNet integration (optional). If network_id is given, we can enable VNet integration and deploy the DB into that VNet (Azure flexible server can be deployed with private access in a subnet). If no network, we can default to public access (which means Azure will create it with a public endpoint, protected by firewall rules). By default, to align with others (which default to private), we might create it with no public access, but then how to access it? Possibly better to allow public by default but locked down by firewall to the client IP or no access until configured. This is tricky; perhaps simpler: if no network given, Azure DB will be public but the provider will set the firewall to allow no IPs (essentially it's inaccessible until user opens). That way it's secure by default. The provider sets admin username and password similar to AWS. Azure requires the username to have a domain in it (like adminuser@servername in connection strings), but we can hide that detail. The engine and version are specified (Azure supports specific versions like MySQL 8.0, PostgreSQL 14, etc.). We will map a generic engine and version to the Azure offering; if exact version not supported, we choose the closest or let Azure default. Sizing in Azure flexible server: you choose a SKU name or core count/memory; we'll map our size input to an SKU (for example, "small" -> 1 vCPU, 2GB). Storage size default maybe 20GB as well.
GCP (Cloud SQL): Creates a Cloud SQL instance for MySQL or PostgreSQL. The provider will specify database version (like "MYSQL_8_0" or "POSTGRES_14") based on engine and version input. Machine type is determined by a combination of tier or custom settings; we map size to either a predefined instance class (like db-f1-micro for smallest, db-n1-standard-1 for small, etc.) or use the newer custom vCPU+RAM settings. Perhaps simplest: "small" -> use a lightweight shared core instance (f1-micro or db-custom-1-3840 for 1 vCPU, 3.75GB?), "medium" -> 2vCPU, "large" -> 4vCPU, etc. The provider sets the user password as given or generates one. Networking: if network_id is provided, we will attempt to connect the Cloud SQL instance to that VPC (via a private IP). In GCP, that means setting up a VPC peering or using a Serverless VPC connector for Cloud SQL? Actually Cloud SQL private IP requires enabling a connection to a VPC (and selecting a subnet). We may need to ensure the provided network has an associated automatic subnet for the region or let Cloud SQL handle it. If no network is given, Cloud SQL by default can have a public IP. We likely default to public but require authorized networks to connect; by default we can leave it locked (no authorized networks). So similar approach: default to no external access unless user opens it. The endpoint (public IP or private IP) will be output.
Schema Design: The abstract_database captures core DB settings:
name – string, optional: Name of the database instance. If not provided, a name is generated. On AWS and Azure, the instance name becomes part of DNS endpoint (AWS uses random endpoint by default, Azure uses server name as DNS name, GCP uses instance name in connection string). We can set the name or let cloud generate one. We’ll use name if given for consistency (must be unique within cloud project/region).
type – string, required: "aws", "azure", or "gcp".
engine – string, required: Database engine type, e.g. "mysql" or "postgresql" (case-insensitive). Possibly allow "postgres" as alias for "postgresql". For extensibility, one might allow "mssql" (SQL Server) in the future if needed, but since not all clouds support it (AWS and GCP do, Azure has its own "Azure SQL" which is similar but not the same API), we can start with MySQL and Postgres.
version – string, optional: Version of the database engine, e.g. "14" for PostgreSQL 14, "8.0" for MySQL 8.0. If not specified, each cloud will default to a current major version. If specified, the provider picks the closest matching available version on that cloud. Minor version differences (like AWS might have 14.5 vs Azure 14.4) are abstracted.
size – string, optional: Generic size tier for the DB instance. Similar concept as instances: e.g., "small", "medium", "large", or numeric hints. This affects the CPU/memory of the DB. The provider maps it to:
AWS instance classes (db.t3.small for small, db.t3.medium for medium, etc., or if larger needed maybe db.m5.large for large).
Azure compute tiers (for flexible server, e.g., 1 vCPU Gen5 for small, 2 vCPU for medium).
GCP machine types (shared core vs standard).
If not provided, default to a small instance (like single CPU with 1-2GB).
storage_gb – number, optional: Storage allocation in GB. Default maybe 20 GB. User can increase this if needed. All clouds allow storage specification (and auto-growth in some cases, but we won't handle auto-grow settings in first pass).
username – string, optional: Master/administrator username for the database. Defaults to a standard name (e.g., "admin" or "root" depending on engine). If provided, must meet the requirements (like no reserved names, etc. – provider will validate).
password – string, optional: Password for the admin user. If not provided, the provider will generate a random strong password (and mark it sensitive). If provided in config, it should be via a sensitive input variable ideally.
network_id – string, optional: Reference to an abstract_network to deploy this database in a private network. If provided:
AWS: The RDS instance will be created in that VPC (the provider will create a subnet group covering the subnets of the given VPC, if not existing, and use that with publicly_accessible = false).
Azure: The DB will be created with VNet integration in that network’s subnet (the user might need to specify which subnet via annotations if multiple, or we pick the first). This implies no public endpoint.
GCP: The Cloud SQL will be set to private IP and linked to that VPC (via a connection).
If network_id is omitted:
AWS: RDS will default to the default VPC, and we could decide publicly_accessible = false (meaning only accessible within default VPC). To allow user to connect from outside (like from their PC), they'd need to either use a VPN/SSH or change it. Since that's advanced, leaving it private is safer. Alternatively, we might default to publicly_accessible = true if no network provided, so it's reachable over the internet (with proper security group rules). But security-wise, not ideal. Perhaps lean towards not public by default on AWS as well.
Azure: If no network, we create a public endpoint DB. Azure requires firewall rules; by default, we don't allow any IP. The user would have to open to their IP via Azure Portal or in future via an annotation.
GCP: If no network, we enable a public IP but with no authorized networks (so only accessible if user manually configures Cloud SQL Proxy or adds network rules).
annotations – map(string->string), optional: Cloud-specific settings:
AWS: instance_class to explicitly set a DB instance class (overrides size), multi_az ("true"/"false") to enable Multi-AZ deployment for high availability, public to override default public accessibility (e.g., "true" to force a public endpoint if desired), parameter_group_name if the user wants a custom parameter group, etc.
Azure: resource_group for the server, sku_name if wanting a specific SKU name, high_availability to enable zone-redundant HA (Azure Flexible Server has an HA option), public (true/false to allow public endpoint if in VNet scenario perhaps).
GCP: tier (like "db-f1-micro") to directly set machine type, availability_type ("ZONAL"/"REGIONAL" for HA), public (to explicitly enable/disable public IP).
We could also allow backup_retention_days, backup_window, but those might be too detailed for initial scope. Possibly skip.
Output attributes:
id: The database instance identifier (AWS RDS instance ARN or name, Azure resource ID, GCP instance connection name).
endpoint: The connection endpoint. Could be a sub-object with host and port, or a formatted "host:port" string. We will provide at least the host (endpoint DNS name or IP) and port (usually 3306 for MySQL, 5432 for Postgres – we know based on engine).
username: The master username (for convenience, if needed; not sensitive).
password: We will not output the password unless we generate it and want to provide it – but safer approach: if user didn't supply a password, we might output the generated one as a sensitive value. However, storing DB password in state is not great. We might instead encourage user to provide it. If we do generate, we could output it as sensitive so they can retrieve and then perhaps rotate manually. We'll document this carefully.
Possibly database_name: If we created a default database (some systems let you specify an initial DB name to create; we didn’t include that, but could via annotation).
Behavior:
Creation time can be several minutes on all clouds (especially if HA). The provider will wait until the instance is available, and then output the endpoint.
Password handling: We'll use Terraform's sensitive features for any password. For example, if generating, we might mark the internal state with the password (encrypted in state if user uses state encryption).
If a user loses the password, they can reset it by changing the password field (which the provider will apply via Modify commands, possibly with a short downtime for AWS/Azure).
Changing engine or major version would force new resource (no automatic engine migration).
Scaling up (size, storage): these can often be done in place (AWS allows instance class change with a reboot, Azure flexible can scale cores, GCP can scale machine type). We might allow increasing size or storage_gb in place, but for simplicity of Terraform lifecycle, we might also force new on size changes in the first iteration to avoid complex wait logic. Perhaps better: storage can increase in place (since all providers support that fairly straightforwardly), instance class can also change in place (with a reboot) – but implementing that requires extra API calls and wait. To keep provider logic lighter initially, we might mark size as ForceNew too. (This is a decision where we trade some efficiency for simplicity; could be revisited in future version).
Deletion: The provider will delete the DB instance. If the user didn’t take a snapshot, data will be lost. Possibly allow an annotation like skip_final_snapshot = true or final_snapshot_identifier = "name" similar to Terraform AWS provider. By default, to avoid surprise data loss, AWS RDS tries to snapshot on delete. We may by default skip snapshot for quick deletion (since this is an abstraction, or maybe do it?). For now, perhaps skip to avoid leaving behind resources. We can document that it's deleting outright.
The provider will ensure that if it created any sub-resources, they are cleaned (like AWS subnet group, Azure if any specific network resource, but likely not needed aside from the DB itself).
Multi-cloud usage: Not much interplay except maybe an app in one cloud connecting to a DB in another via the endpoint. The abstraction ensures each DB can be referenced for its endpoint and credentials as needed.
Multi-Cloud Deployment and Interoperability
One of the core benefits of the Abstract Cloud Provider is the ability to mix and match cloud resources in a single Terraform configuration – so-called hybrid deployments. Terraform inherently supports multiple providers in one config, and here we leverage that to allow a single provider (with internal multi-cloud capabilities) to provision resources across AWS, Azure, and GCP seamlessly. Single State, Multiple Clouds: All abstract resources, regardless of target cloud, are managed under the umbrella of the abstract provider. This means Terraform retains one state file that contains all these resources. Users do not need to initialize or maintain separate providers for each cloud (though behind the scenes the abstract provider itself handles multiple APIs). During a plan/apply, Terraform will orchestrate creation and destruction in the correct order respecting dependencies. For example, you could have an AWS bucket and an Azure function in the same config; if the function’s configuration depends on the bucket (say, using the bucket’s name in an environment variable), Terraform will create the bucket first then the function, even though they are different clouds, because the resource graph knows the dependency. Cross-Resource References: Because outputs from any abstract resource can be used as inputs to any other, users can wire components across clouds. For instance, an Azure function could read from or write to an AWS S3 bucket by using the bucket’s URL or name, which can be passed as an environment variable. In Terraform configuration, this looks like normal interpolation syntax:
resource "abstract_bucket" "data" {
  type = "aws"
  name = "my-data-bucket"
  versioning = true
}

resource "abstract_function" "processor" {
  type    = "azure"
  name    = "data-processor"
  runtime = "nodejs14"
  code    = "function_code.zip"
  environment = {
    BUCKET_NAME = abstract_bucket.data.name   # cross-cloud reference
  }
  trigger_http = true
}
In the above, abstract_bucket.data.name (which would be "my-data-bucket") is passed into the Azure Function's environment. The function code (running on Azure) can then use that value to call the AWS bucket (assuming appropriate access credentials are handled in code or via some IAM trust). The Terraform provider’s job is simply to ensure the data is flowed correctly. There is no inherent restriction – you could reference the output of an AWS resource in a GCP resource or any combination. The only caveat is that these references typically are just identifiers or endpoints; actual connectivity or permissions between the resources is outside Terraform's scope (e.g., to let an Azure function access an AWS bucket, you might need to provide AWS credentials to it or make the bucket public, which is a design choice left to the user). Provider Configuration for Multi-Cloud: As described earlier, the user needs to configure credentials for each cloud that will be used. They can choose to configure only AWS, for example, if no Azure or GCP resources are in the config. The provider will error at plan time if a resource is defined for a cloud that has not been configured. It's also possible to target multiple accounts or subscriptions for the same cloud by using multiple provider instances with aliases (like provider "abstract" { alias = "aws2" ... } with a different AWS credential), but that complicates usage and isn't a primary goal. Typically, one abstract provider instance corresponds to one set of AWS/Azure/GCP credentials at a time. Ordering and Dependencies: Internally, the provider does not need special handling for cross-cloud ordering – Terraform’s core engine sees all resources and their references. For example, if an abstract_database on GCP is needed before an abstract_function on AWS (maybe the function takes the DB endpoint), Terraform plan will establish that dependency and apply in correct order. The provider ensures that any attribute that might be needed is correctly exposed. We must ensure outputs that are needed across clouds are of compatible types (strings, etc.). For instance, an AWS bucket name is a plain string which can be passed anywhere. A more complex example: if an AWS Lambda needed to be triggered by a GCP pub/sub (not likely in our abstraction because triggers are not cross-cloud), that would be complex – but since we focus on loosely coupled components via data (names, URLs), we avoid entangling two cloud resources in one resource’s API. Handling Cloud-Specific IDs: When passing references, sometimes an ID is only meaningful in the context of that cloud (like an AWS VPC id has no meaning in Azure). We anticipate that and generally one would not pass such IDs across clouds unless it’s something like a network that exists in both (which doesn’t happen; you can't attach an Azure VM to an AWS VPC, as a forum post noted
discuss.hashicorp.com
). Our abstraction avoids that scenario by design (for example, each instance/cluster references a network of the same cloud). So, cross-cloud references will typically be things like strings or URLs that are just data. It's up to the user to use them appropriately (the provider isn't validating usage beyond Terraform’s normal type checking). Unified Logging/Outputs: Each resource will log its creation in Terraform separately. There isn’t a combined cross-cloud orchestration layer besides Terraform itself. Users will see, for example, an AWS API call trace for the bucket and an Azure API call trace for the function during terraform apply. The provider may group some actions (like for Azure function it creates storage account then function app, etc., but it will treat it as part of one Terraform resource). From the user's perspective, they just see one plan with all resources. State and Backends: Because multi-cloud deployments will often be used by teams to manage global infrastructure, the state file can become sensitive (containing all cloud resources). Storing state in a remote backend (like Terraform Cloud or an encrypted storage) is recommended, especially since it may include sensitive outputs (like DB passwords or function secrets). This is not different from normal Terraform usage, just a reminder that because multiple clouds are managed together, losing the state or having it corrupted could affect resources in all clouds at once. Example – Hybrid Deployment Scenario: Below is a sample Terraform configuration that showcases a hybrid deployment using the Abstract Cloud Provider. It sets up an AWS S3 bucket and an Azure Function that processes data from that bucket, as well as a GCP MySQL database that the function writes to. This contrived example demonstrates multi-cloud coordination:
provider "abstract" {
  aws = {
    region     = "us-east-1"
    access_key = "AKIA...YOURKEY"
    secret_key = "YOURSECRET"
  }
  azure = {
    subscription_id = "xxxx-xxxx-xxxx-xxxx"
    client_id       = "azure-client-id"
    client_secret   = "azure-secret"
    tenant_id       = "azure-tenant-id"
    location        = "eastus"
  }
  gcp = {
    project    = "my-gcp-project"
    region     = "us-central1"
    credentials = file("gcp-key.json")
  }
}

# 1. AWS S3 Bucket for data storage
resource "abstract_bucket" "data_bucket" {
  type       = "aws"
  name       = "myapp-data-bucket-123"   # must be globally unique
  versioning = true
  # AWS-specific: make bucket private (default behavior). Ensure we can delete it if not empty:
  annotations = {
    force_destroy = "true"
  }
}

# 2. GCP MySQL Database for persistent storage
resource "abstract_database" "app_db" {
  type     = "gcp"
  engine   = "mysql"
  version  = "8.0"
  size     = "small"
  storage_gb = 20
  username = "dbuser"
  password = var.db_password  # sensitive input variable
  # No network_id -> will be accessible via public IP (but no authorized networks by default)
}

# 3. Azure Function that processes data from S3 and stores results in the DB
resource "abstract_function" "processor" {
  type        = "azure"
  name        = "data-processor-fn"
  runtime     = "python3.9"
  code        = "${path.module}/function_code.zip"  # Package of function code
  handler     = "process.handle"        # e.g., in code, use process.py with function handle()
  memory      = 256
  timeout     = 30
  trigger_http = true                   # expose HTTP endpoint
  environment = {
    "BUCKET_NAME"   = abstract_bucket.data_bucket.name        # S3 bucket name (for AWS SDK usage in code)
    "DB_HOST"       = abstract_database.app_db.endpoint.host  # Database host from GCP
    "DB_PORT"       = tostring(abstract_database.app_db.endpoint.port)
    "DB_NAME"       = "appdata"
    "DB_USER"       = abstract_database.app_db.username
    "DB_PASSWORD"   = var.db_password                         # pass through same DB password (could also use abstract_database.app_db.password if output)
  }
  annotations = {
    resource_group = "rg-serverless"    # place function in this RG
    # The Azure function code is assumed to read BUCKET_NAME and connect to AWS using some credentials.
    # Those credentials could be set via environment or managed via Azure Key Vault (not shown here).
  }
}
Expected Behavior: When the user runs terraform apply on this configuration, the provider will perform the following steps (high-level):
AWS S3 Bucket – Create a new S3 bucket named "myapp-data-bucket-123" in us-east-1. Enable versioning on it. Since force_destroy was true, the provider will mark the bucket to allow deletion even if objects exist (by auto-emptying it on destroy). The bucket’s ARN and name are saved in state, and its name is output for use by others.
GCP MySQL Database – Create a Cloud SQL instance with MySQL 8.0 in us-central1. It will have a name like "app-db" (perhaps derived from abstract_database.app_db.name or generated). No VPC is specified, so it gets a public IP but no authorized networks – meaning it's reachable if you have the correct user/password and either using the Cloud SQL Proxy or you add network auth. Terraform will output the instance’s connection endpoint (something like app-db.us-central1.cloudsql.googleapis.com and port 3306) and credentials. The password is from var.db_password in this case, so known to the user, and not stored in plaintext in state (Terraform will mark the variable sensitive and not log it).
Azure Function – The provider will:
Ensure the resource group "rg-serverless" exists (if not, it might implicitly create it or throw an error if Azure APIs require explicit creation – ideally, the user would create an abstract_network or some Azure resource group abstraction, but for simplicity we allow the provider to auto-create the RG if needed).
Create a Storage Account for the function (e.g., named something like funcdpp5abcd since Azure needs one for function app).
Create a Function App (with a new Consumption Plan) named "data-processor-fn". It will set it to Python 3.9 runtime.
Deploy the code from function_code.zip to that Function App. The handler = "process.handle" might translate to Azure's function entry (Azure might use a different mechanism, but we'll assume this gets handled either by a function.json or by convention).
Configure Application Settings for the environment variables: BUCKET_NAME, DB_HOST, etc., injecting the S3 bucket name and DB endpoint that were created earlier.
Because trigger_http is true, configure an HTTP trigger for the function. This might mean Azure will have a function named "data-processor-fn" with an HTTP trigger with anonymous access. The provider ensures the function URL (something like https://data-processor-fn.azurewebsites.net/api/data-processor-fn) is retrieved.
Output the invoke_url for the function.
The function code, when executed (not part of Terraform’s concern), would use the BUCKET_NAME to fetch data from S3 (assuming it has AWS credentials, which could be set via environment or implicitly if using public bucket), then use DB_HOST/DB_USER etc. to connect to the MySQL DB and store results. This is beyond Terraform, but our provisioning made it possible to configure such integration.
Terraform finishes, reporting success. It would list the resources created: one AWS bucket, one GCP database, one Azure function. The Azure function’s URL might be output, which the user could then hit to trigger processing.
This scenario demonstrates that our provider can facilitate a multi-cloud application: data in AWS, compute in Azure, database in GCP – all defined in one cohesive config. The interpolation of values (like passing bucket name and DB host) is handled naturally by Terraform. The provider didn’t need special logic except providing those output fields. Multi-Cloud Differences: Users should be aware that not everything is perfectly uniform – for example, the Azure function’s cold start and scaling behavior might differ from, say, an AWS Lambda if we had one. But those details are at the cloud service level, beyond the abstraction. We focus on making sure the provisioning and wiring of resources is smooth.
Extensibility and Future Improvements
Designing an abstract provider involves careful consideration of scope. This section outlines how the provider can be extended in the future, and constraints to keep in mind: Adding New Resource Types: In the future, we may expand to other common services as long as they fit the lowest common denominator model. For example:
Queues/Topics: A messaging service abstraction (AWS SQS, Azure Storage Queues or Service Bus, GCP Pub/Sub). These are available in all three clouds, though with some differences. We could introduce an abstract_queue resource mapping to those. The schema might include attributes like FIFO vs standard (AWS has FIFO, others might not) so we’d stick to basics.
Load Balancer: All clouds have load balancers. We could abstract a simple L4/L7 load balancer that points to instances or functions. However, this might require also abstracting the target attachment, which gets complex.
Container Registry: All have container registries (ECR, ACR, GCR). A generic abstract_registry could manage a private image registry.
DNS Zones/Records: As noted in HashiCorp docs, DNS is fairly portable
developer.hashicorp.com
. An abstract_dns_record resource could be feasible to map to Route53, Azure DNS, Cloud DNS.
Secrets Management: Perhaps a stretch, but all three have secrets vaults (AWS Secrets Manager, Azure Key Vault, GCP Secret Manager). A unified abstract_secret might be possible.
When adding a resource, the steps would be:
Confirm the service exists in AWS, Azure, GCP in a comparable form.
Define a minimal common schema that covers the essential functionality.
Implement create/read/update/delete for each cloud’s API.
Document any annotations for edge cases or extra features.
We would maintain the design principle that if a resource’s concept doesn’t cleanly map to one of the clouds, we exclude it to avoid a poor abstraction. For example, serverless container services (AWS Fargate on ECS, Azure Container Instances, GCP Cloud Run) might be considered for abstraction since they are similar – that could be a future addition if demand exists. Supporting Additional Cloud Providers: While AWS, Azure, and GCP cover the majority of use cases, the provider could potentially support more clouds (with the understanding that "abstract" might lose meaning if it becomes too broad). If we were to add, say, Oracle Cloud or IBM Cloud in the future:
The provider type field could accept new values like "oci", "ibm", etc.
We would implement the same resource types for those clouds as much as possible. For example, if adding Oracle OCI: it has buckets (Object Storage), instances (Compute VM), maybe an equivalent of functions (Oracle Functions), and a managed database (Autonomous DB or MySQL). We’d map accordingly or decide if any resource isn’t supported on that cloud (then the provider should reject it if used with that type).
The provider configuration would need to accept credentials for the new cloud (e.g., an oci {} block with necessary keys).
We must ensure not to break existing users; adding new type options is backward-compatible, but we need to test that adding new cloud logic doesn’t interfere with others.
Modular Codebase: Internally, to ease future extensions, the provider code should be organized by cloud and resource. For example, we might have packages or files like aws_bucket.go, azure_bucket.go, gcp_bucket.go implementing each variant’s API calls, with a common interface so that the resource code simply calls CreateBucket(ctx, data, meta) and the implementation dispatches to the correct cloud-specific function. This modular approach allows contributors knowledgeable in one cloud to add improvements or new features for that cloud’s handling without affecting others. Annotations vs Schema Evolution: We anticipate that over time, some features currently hidden behind annotations might become mainstream enough across all clouds to promote to top-level schema fields. For instance, if all three clouds introduce a similar concept or if we initially omitted something like auto-scaling configuration for clusters and later find a uniform way to support it, we can add a new field (say enable_autoscaling, with subfields for min/max nodes) to the abstract_cluster resource. When doing so:
We maintain backward compatibility: existing configs using annotations would still work; the provider would just prefer the new first-class field if set.
We update documentation and encourage users to use the new field for clarity.
Backward Compatibility and Versioning: Changes to the provider will follow semantic versioning. Because it’s likely to be used in production IaC, we will be cautious with breaking changes. For example, if we realize a default we chose is not ideal, we may have to keep it for existing behavior and perhaps introduce a new option to change it. We will leverage Terraform’s Deprecated flags for schema if we ever need to rename or phase out a field or annotation key. The design from the start aims to minimize such churn by choosing clear names and patterns. Handling Provider Updates: We must keep track of cloud provider API changes. For instance, new AWS regions or Azure introduces a new runtime or deprecates an old one. The provider should periodically update the mappings (like available runtime values, instance types mapping, etc.). We could implement some of these as data files or small generated code to ease updates. Also, if any of the three providers starts supporting a feature that was missing (making it now a common denominator), we could move that feature from an annotation to a core field in a new version. Performance Considerations: With the provider orchestrating potentially many API calls (especially for complex resources like clusters or functions), we need to consider timeouts and parallelism:
The Terraform plugin SDK typically handles each resource create in its own routine. Our provider might be hitting three different APIs concurrently if resources are created in parallel. This is usually fine, but we should ensure thread-safety for any shared client objects. Likely we will not share clients across resources without locks, or we will create separate clients per resource invocation.
Long-running operations (like EKS cluster creation) might need increased timeouts. We will set appropriate timeouts in the provider (Terraform allows configuring a timeout on operations; we can set default and allow override via schema if needed).
We might implement some simple retry logic for transient errors on API calls to improve robustness across cloud calls.
Security and Credentials: Future improvements could include tighter integration with each cloud’s auth options:
For example, allowing the AWS credentials to be picked up from the environment or EC2 instance profile if Terraform runs in AWS – which we likely will support by default as the underlying AWS SDK does so. Similarly for Azure (managed identity) and GCP (if running on GCP).
We might also consider features like assuming roles or using Azure Managed Service Identity via annotations if needed to deploy in certain constrained environments.
Another security consideration: storing sensitive data in state (like generated passwords). We will document best practices (e.g., if a password is output, to mark it sensitive or use Vault integration). Possibly in future, integrate with Vault or other secret stores to avoid cleartext secrets in state (beyond Terraform's own mechanisms).
Community Feedback and Flexibility: Finally, being an abstraction layer, we anticipate that not all use cases can be covered. The provider should make it easy to combine with regular provider resources if needed. For example, if a user needs an advanced S3 bucket feature not supported by abstract_bucket, they could create that bucket with the AWS provider separately and perhaps feed its name into other abstract resources. Or vice versa, someone might create a base infrastructure with abstract provider and then tweak one cloud’s resource with cloud-specific provider if necessary. Our design doesn’t prevent using the official AWS/Azure/GCP providers alongside – they just would manage separate resources. We will encourage feedback and perhaps implement toggles to relax or change behavior when it makes sense. For instance, if many users request the ability to have one Azure storage account hold multiple buckets to reduce cost, we could allow an annotation to specify an existing storage account id (and not delete it on destroy if not created by us). These kind of enhancements can be layered without affecting those who don’t use them.
Conclusion
The Abstract Cloud Provider for Terraform offers a unified, cloud-agnostic way to define core infrastructure primitives that span AWS, Azure, and Google Cloud. By embracing a lowest-common-denominator philosophy
developer.hashicorp.com
 and carefully mapping to each cloud’s services, it enables a dramatic simplification for organizations aiming for multi-cloud deployments. The design balances abstraction with flexibility: common infrastructure patterns are easy to deploy, while cloud-specific nuances can still be accessed via annotations when absolutely needed. Through clearly defined schemas for Cluster, Instance, Bucket, Function, Database, and Network resources, this provider covers the foundational pieces for cloud-agnostic application architecture. We illustrated how these resources correspond to each cloud’s offerings and provided examples of usage, including a hybrid scenario spanning all three providers. Implementation in Go
hashicorp.com
 ensures we can leverage Terraform’s proven provider SDK and each cloud’s official libraries for robust operation. The design handles multi-cloud interpolation naturally, letting Terraform orchestrate dependencies across disparate platforms in one plan/apply cycle. By focusing on portable services, we avoid the pitfalls warned by industry experts about trying to abstract everything
discuss.hashicorp.com
reddit.com
. Instead, we provide a pragmatic set of capabilities sufficient for many common workloads (web services, data processing pipelines, etc.) that need the flexibility of multi-cloud or the simplicity of a single config that can target any cloud environment. Moving forward, the provider can grow to support more resources and even additional clouds, guided always by the principle of consistent abstraction – only expanding where a truly analogous capability exists across platforms. This design document will serve as a blueprint for the implementation phase, ensuring that developers (and tools like Codex) have a clear specification to follow for building out the provider’s functionality. With this foundation, the Abstract Cloud Provider will empower users to write once and deploy anywhere, fulfilling the promise of cloud-agnostic infrastructure as code. Sources:
HashiCorp Terraform documentation on multi-cloud abstractions and the lowest common denominator approach
developer.hashicorp.com
.
Community discussions on cloud-agnostic Terraform design, highlighting the feasibility of abstracting services like Kubernetes clusters
discuss.hashicorp.com
 and the challenges of overly generic solutions
discuss.hashicorp.com
.
HashiCorp blog on Terraform providers indicating Go as the primary implementation language
hashicorp.com
.

Favicon
Favicon
Favicon
Favicon
Sources



No file chosenNo file chosen
ChatGPT can make mistakes. Check important info.

Design Document for an Abstract Multi-Cloud Terraform Provider
ChatGPT
OK, let's align specific cloud technologies like clusters, functions, and storage buckets with universal constructs to keep things simple and consistent.

Searched for cloud-agnostic Terraform provider resources
ChatGPT
I’m analyzing cloud providers' low-common-denominator constructs to shape a Terraform abstraction design, ensuring alignment with foundational services like compute, storage, and networking.

Searched for Terraform multi-cloud abstract resources
ChatGPT
Crossplane offers cloud-agnostic resource definitions in a Kubernetes-like style, which might be a unique abstraction approach.

Favicon
Read tekanaid.com

ChatGPT
I'm looking into a link that isn't opening, trying different solutions. Progress is steady, but I'm still assessing options.

Searched for Crossplane multi-cloud abstract resources
ChatGPT
I’m noting a Reddit post discussing Terraform abstraction, a promising exploration of provider-independent methods. This aligns with the topic's relevance.

Favicon
Read reddit.com

ChatGPT
Creating an "Abstract Cloud Provider" involves mapping lowest common denominator resources such as compute clusters, storage buckets, and serverless functions across AWS, Azure, and GCP.

I'm inclined to interpret "cluster" as a Kubernetes cluster, considering the multi-cloud context and the need for a consistent environment across AWS, Azure, and GCP.

Searched for lowest common denominator excluding AWS Aurora
ChatGPT
Spanning cloud integration, multi-cloud interpolation, and extensibility, the aim is to unify diverse resources under a single provider for seamless multi-cloud deployments.

I’m refining how annotations manage cloud-specific info. A table groups resources by cloud type, mapping them to nuances like AWS abstract_bucket to Azure's abstract_bucket.

I’m combining single cloud and hybrid configurations, highlighting ease in switching cloud types using variables, and ensuring future extensibility of resources.

Searched for Terraform provider design Go schema
ChatGPT
I’m gathering info on Terraform schemas from the HashiCorp page to understand their definition. This leads me to a clearer context for custom provider guidelines.

Favicon
Read hashicorp.com

ChatGPT
Terraform's 'provider' logically represents an upstream API, crucial for managing configurations.

Favicon
Read more from hashicorp.com

ChatGPT
OK, let me see. Plugins are primarily Go binaries, with rare exceptions. This aligns with Terraform's standard practice.

Favicon
Read dev.to

ChatGPT
I'm thinking through cloud-agnostic modules for AWS and Azure, contrasting module-based approaches with direct provider-based techniques.

Favicon
Read discuss.hashicorp.com

ChatGPT
It's interesting to see that the typical method uses platform-specific modules, but the emphasis here is on clarity. Additionally, considering the impact of cross-cloud abstraction.

Favicon
Read dev.to

ChatGPT
I'm considering how to abstract and manage AWS, Azure, and GCP resources seamlessly by choosing defaults like using the latest Ubuntu LTS image or a specific node count for clusters.

I'm weighing HTTP triggers for AWS, Azure, and GCP functions. Ensuring they can invoke via HTTP endpoints aligns with common use cases across these platforms.

Favicon
Read discuss.hashicorp.com

ChatGPT
I’m thinking through the challenge of abstracting a single VM instance versus a high-level abstraction like a Kubernetes cluster.

Favicon
Read developer.hashicorp.com

ChatGPT
Terraform deliberately avoids abstracting cloud services to maintain full feature exposure. Users can create custom abstractions by balancing essential features.

Favicon
Read more from developer.hashicorp.com

ChatGPT
I’m finishing the design document with a detailed table for resource mapping, examples of hybrid deployments, and a section on extensibility. Progressing smoothly towards completion.

I’m noting that critical fields like bucket name and type necessitate creating a new resource due to AWS and GCP limitations. Progress is steady and methodical.

Research completed in 9m · 10 sources

Preview


Download PDF

Share Link
Abstract Cloud Terraform Provider – Design Document
Introduction
The Abstract Cloud Provider is a custom Terraform provider that enables defining cloud-agnostic infrastructure resources which can be deployed across AWS, Azure, and GCP. Instead of writing separate Terraform configurations for each cloud, users can use generic resource types (e.g. Cluster, Bucket, Function, etc.) with a type field indicating the target cloud. The provider translates these abstract resources into the appropriate cloud-specific resources under the hood. By focusing on services common to all major providers, this abstraction follows a deliberate "lowest common denominator" approach
developer.hashicorp.com
 – avoiding cloud-specific features (for example, AWS Aurora or Azure Cosmos DB) in favor of broadly supported primitives. This design document outlines the provider's architecture, the schema of each resource, mappings to underlying cloud services, handling of multi-cloud deployments, and considerations for future extensibility. The goal is to provide a consistent user experience and configuration interface, while the provider takes on the complexity of provisioning the correct resources in each cloud. All implementation will be done in Go, as is standard for Terraform providers
hashicorp.com
, leveraging official cloud SDKs or APIs. The result will allow a single Terraform configuration to provision infrastructure on multiple clouds simultaneously (hybrid deployments), or to easily target a different cloud by changing a parameter, without rewriting the entire configuration.
Design Philosophy and Constraints
Cloud-Agnostic Abstraction: Terraform itself does not natively abstract across vendors, to avoid hiding unique features behind least-common interfaces
developer.hashicorp.com
. This provider intentionally creates a simplified abstraction, meaning we only include features that are present in all three clouds. This inevitably means sacrificing some advanced capabilities in exchange for portability. For example, we support a generic Database resource for basic relational databases, but not specialized offerings like Amazon Aurora or Spanner which have no equivalent in other clouds.
Lowest Common Denominator Features: Each abstract resource type encapsulates the minimal set of properties needed to provision a functional resource on any cloud. If a feature or setting cannot be mapped to all three providers in a reasonably equivalent way, it will not be a first-class field in the schema. Instead, such settings can sometimes be passed via the annotations map (see below) for provider-specific tweaks when absolutely necessary. This design avoids the complexity and cost of trying to emulate cloud-specific functionality on other platforms
discuss.hashicorp.com
.
Trade-offs and Portability: Users of the Abstract Cloud Provider must understand the trade-off: by using a single interface for multiple clouds, they gain consistency and reduce duplication, but they also accept a loss of cloud-specific optimizations. The design prioritizes portability and simplicity over maximizing each cloud’s capabilities. As noted in community discussions, fully abstracting even basic resources like VM instances can be challenging due to subtle differences
reddit.com
. Our approach mitigates this by focusing on higher-level constructs (e.g. managed clusters, serverless functions, object storage) where the cloud implementations are more analogous
discuss.hashicorp.com
.
Resource type Field: Each resource requires a type attribute to specify the target cloud provider (e.g. "aws", "azure", or "gcp"). This flag determines which cloud-specific API calls or Terraform configurations the provider will use. The type field is immutable after creation – changing it forces the destruction and re-provisioning of the resource in the new cloud (marked as ForceNew in schema), since Terraform cannot migrate an existing resource across providers.
annotations for Cloud-Specific Settings: Every resource schema includes an optional annotations map for passing through cloud-specific settings or flags that are not part of the core abstract schema. This allows flexibility when a particular cloud requires an extra parameter or the user wants to opt into a cloud-specific feature without breaking the overall abstraction. For example, an abstract_bucket might accept an annotation for an AWS-only setting like acl = "private" without introducing an acl field globally. The provider will interpret the annotations in the context of the specified type and ignore irrelevant keys. The use of annotations is meant as an escape hatch; the provider will document supported annotation keys for each cloud resource type. Importantly, reliance on too many annotations may reduce portability, so the design encourages using them sparingly – the common-case usage should not require annotations at all.
Hybrid Deployment Support: The provider is designed to handle configurations that include resources targeting different clouds in one Terraform run. For example, a single configuration could create an AWS S3 bucket, an Azure Function, and a GCP database together. Terraform will manage the dependency graph normally – an abstract resource can reference another abstract resource’s attributes regardless of cloud, since all resources are managed by the one provider. No special handling is needed beyond ensuring credentials for all target clouds are available. We do ensure that references between resources only use common output attributes (e.g. using a bucket’s name or URL in a function’s environment variable) rather than cloud-specific identifiers that wouldn’t make sense cross-cloud. The provider does not attempt to directly connect resources across clouds (for instance, attaching an AWS Lambda to an Azure storage trigger), but it makes it straightforward to compose such multi-cloud architectures by sharing data through Terraform outputs and variables. In essence, interpolation of values between clouds is fully supported (e.g., using an AWS-generated resource URL in an Azure resource configuration), but it’s up to the user to use those values meaningfully (such as coding a function to call an external service in another cloud).
Terraform Provider Implementation: The provider will be implemented in Go (using HashiCorp’s Terraform Plugin SDK v2 or the newer Terraform Plugin Framework). This is the standard and recommended practice for Terraform providers
hashicorp.com
, ensuring we can compile the provider to a single binary plugin. Each abstract resource corresponds to a resource implementation in the provider code with CRUD (Create, Read, Update, Delete) operations. Internally, those operations will invoke the respective cloud’s SDK/API calls based on the type. For example, the Bucket resource’s create function will have logic like: if type=="aws" call AWS S3 API to create a bucket; if type=="azure" call Azure Storage SDK to create a storage account and container; if type=="gcp" call GCP Cloud Storage API to create a bucket, and so on. The provider will manage multiple cloud API clients under the hood (one for AWS, one for Azure, one for GCP), initialized with credentials from the provider configuration. The state stored by Terraform will contain the necessary identifiers for each cloud’s resources (like ARNs, Azure resource IDs, etc.), but the user will mostly interact with high-level outputs (names, URLs, etc.). By structuring the code with clear separation for each cloud’s logic (for maintainability), we ensure adding a new cloud or updating an API is straightforward.
Provider Configuration (Credentials): To use this provider, the user must configure access to each cloud they intend to use. The provider supports authenticating to AWS, Azure, and GCP. This can be done via environment variables (e.g. AWS_ACCESS_KEY_ID, Azure client credentials, GCP credentials JSON, etc.) or via an abstract provider configuration block in Terraform. For instance, the provider config may allow specifying credentials for each cloud and default settings like default region:
provider "abstract" {
  aws = {
    region     = "us-west-2"
    access_key = var.aws_access_key
    secret_key = var.aws_secret_key
  }
  azure = {
    subscription_id = var.azure_sub_id
    client_id       = var.azure_client_id
    client_secret   = var.azure_client_secret
    tenant_id       = var.azure_tenant_id
    location        = "westus2"
  }
  gcp = {
    project    = var.gcp_project
    region     = "us-central1"
    credentials = file("gcp-key.json")
  }
}
The above shows a possible configuration interface: credentials and a default region/location per cloud. The provider will use these defaults when a resource’s region (or location) is not explicitly specified. Each resource can also override the region in its schema if needed (for example, to place one bucket in a different region). If a cloud’s credential block is omitted, the provider will attempt to find credentials from environment or well-known locations (e.g., ~/.aws/credentials or Google Application Default Credentials), similar to official providers.
Naming and Global Uniqueness: Where applicable, the provider will document naming requirements that differ by cloud. For example, S3 and GCS bucket names must be globally unique and DNS-compatible, whereas Azure storage container names must be unique within a storage account. The abstract resources will try to enforce the superset of these rules (e.g., allowed characters) to prevent user error. If a name is valid for one cloud but invalid for another, the provider may either automatically adjust it (with a predictable transformation) or produce a validation error if the type is known. The goal is to avoid runtime errors from clouds by catching configuration issues in Terraform plan/apply phase.
Resource Mapping Strategy: For each abstract resource, the provider might need to create multiple underlying cloud resources. The user perceives a single resource, but for example, an Azure Function requires not only the function itself but also a storage account and a hosting plan. The provider will manage these ancillary resources internally, either by creating them implicitly or by expecting certain annotations. The design leans toward simplicity: if a cloud’s service requires a supporting resource that has no equivalent concept in others, the provider will attempt to manage it automatically (to avoid burdening the user with cloud-specific setup). For instance, Azure requires a Resource Group for any resource – the provider can either use a default resource group name (e.g., provided in the provider config or a global default like "abstract-provider-rg") or let the user specify one via annotations.resource_group. In AWS, a default VPC might be used for networking if none is specified, whereas in Azure/GCP the provider might auto-create a VNet or use the cloud’s default network. These decisions are detailed per resource below. The guiding rule is: if a reasonable default or automatic step exists to bridge a cloud-specific requirement, the provider will implement it, so that the user’s experience remains as cloud-neutral as possible.
Supported Abstract Resources and Cloud Mappings
The Abstract Cloud Provider will support the following resource types initially, each corresponding to analogous services in AWS, Azure, and GCP:
Abstract Resource	AWS Equivalent	Azure Equivalent	GCP Equivalent
Network	VPC (Virtual Private Cloud)	Virtual Network (VNet)	VPC Network
Instance	EC2 Instance	Azure Virtual Machine (VM)	Compute Engine VM Instance
Cluster	EKS Kubernetes Cluster (plus node group)	AKS Kubernetes Service (managed cluster)	GKE Kubernetes Cluster
Bucket	S3 Bucket	Azure Blob Storage Container (in a Storage Account)	Cloud Storage Bucket
Function	Lambda Function	Azure Function (Function App + Function)	Cloud Function
Database	RDS Database Instance (MySQL/PostgreSQL)	Azure Database for PostgreSQL/MySQL (Single Server/Flexible Server)	Cloud SQL Instance (MySQL/PostgreSQL)
Each abstract resource provides a normalized interface to create the above services. Below we detail each resource’s schema and behavior, including how it maps to each cloud provider.
Abstract Network
Description: An abstract_network resource represents a virtual network for isolating resources and defining IP ranges. This maps to a VPC in AWS, a Virtual Network in Azure, and a VPC Network in GCP. All three clouds share the concept of a private network with subnets, although their defaults differ. The abstract provider uses this resource to allow users to create network infrastructure that other resources (like Instances or Clusters) can attach to, if they want control over IP ranges or segmentation. Cloud mappings:
AWS: Creates a new VPC. By default, it will also create a couple of default subnets (e.g., one per availability zone in the chosen region) and an internet gateway, mimicking AWS’s default VPC structure. If the user provides a cidr (CIDR block), that will be used for the VPC’s address space; otherwise a default like "10.0.0.0/16" may be assumed. The resulting VPC ID is tracked. Any Instance or Cluster of type AWS can reference this network by ID to launch instances in it (the provider will then place those instances in the created subnets). If no network is specified for an AWS resource, the AWS default VPC (if available in the account) could be used as a fallback.
Azure: Creates an Azure Virtual Network in a specified resource group. The cidr will define an address space for the VNet (default could be "10.0.0.0/16" if not provided). The provider will also create a default Subnet within that VNet (e.g., using the same CIDR or a /24 slice of it) since Azure VMs/AKS require at least one subnet. The Virtual Network’s ID and the default Subnet ID are recorded. If the user doesn’t specify a resource group via annotations.resource_group, the provider will use a global one from provider config or create one (e.g., "abstract-default-rg"). Other Azure resources can then be placed in this network/subnet if specified.
GCP: Creates a VPC Network (in auto-mode or custom-mode). By default, we might choose auto mode to automatically create subnets in each region (to mimic how GCP’s default network works). If a cidr is given, we create a custom-mode VPC with a single subnet covering that CIDR in the specified region (or if no region given, perhaps default region from provider config). The network’s self-link or ID is saved. GCP instances or GKE clusters can use this network if provided; otherwise, the default VPC in the project is used by default.
Schema Design: The abstract_network resource schema focuses on defining the network range and optional parameters:
name – string, optional: Name for the network. This might be used as the cloud network name/tag. If not provided, a default name can be generated. (Azure requires a name for VNet, AWS VPC can have a Name tag, GCP network needs a name.)
cidr – string, optional: The IP address range in CIDR notation for the network. Example: "10.50.0.0/16". If not set, the provider uses a sensible default (which could differ per cloud, e.g., AWS default VPC uses 172.31.0.0/16; we might just use 10.0.0.0/16 universally for new networks).
type – string, required: The target cloud ("aws", "azure", or "gcp").
annotations – map(string->string), optional: Cloud-specific settings.
For AWS, annotations might include keys like enable_dns_support or enable_dns_hostnames to control VPC settings (if not, defaults to true).
For Azure, could specify resource_group if the network should be created in a particular resource group, otherwise uses provider default.
For GCP, perhaps auto_create_subnetworks (true/false) to choose auto or custom mode (default to true for auto-mode if no CIDR given).
Output attributes: The network resource would export at least an id (which could be the VPC ID, VNet resource ID, or GCP network name/URI). It may also export subnet_id (the ID of a default subnet created, if applicable) and potentially cidr (normalized) or other properties like gateway_id (AWS internet gateway) if needed for routing.
Behavior and Constraints: Changing the cidr or type of a network after creation will force a new network (since resizing or migrating networks is not generally possible without recreation). The provider will ensure that for each cloud a minimal viable network exists after creation:
AWS: VPC + at least one subnet + internet gateway (so that instances can have internet connectivity by default; security rules would still default to closed).
Azure: VNet + at least one Subnet.
GCP: VPC + subnets (auto or custom as determined).
If users want more advanced network setups (multiple subnets, specific routing, peering, etc.), those are beyond this abstraction’s scope – they would either use cloud-specific Terraform resources or future enhancements of this provider. The abstract_network is just enough to support deploying the compute resources in an isolated network.
Abstract Instance
Description: An abstract_instance resource represents a single virtual machine (VM) instance with basic settings, mapping to an EC2 instance on AWS, an Azure VM, and a Google Compute Engine VM on GCP. This is a fundamental IaaS building block. The abstraction covers launching a VM with a certain machine size and operating system in a specified region. Cloud mappings:
AWS: Creates an EC2 instance. If an abstract_network (VPC) is provided via a network_id or similar field, the instance will be created in that VPC (and in a subnet of that VPC, possibly the first available subnet). Otherwise, it will launch in the default VPC of the AWS account for the region. The provider will choose an AMI if the user hasn’t specified one (for example, using a default Amazon Linux 2 or Ubuntu image in that region). It will also create a basic Security Group to allow the instance to communicate (by default, perhaps open to outbound and allow no inbound except maybe SSH if a key is provided). If the user supplies annotations.key_name (an existing SSH key name) or similar, it will associate that for SSH access. Otherwise, AWS instance will have no key (not accessible via SSH unless user uses AWS Session Manager).
Azure: Creates an Azure Virtual Machine in a given location (and resource group). Azure requires more supporting resources: a Virtual NIC (network interface) attached to a subnet, a disk, and potentially an NSG (network security group). The provider will handle these: if network_id is provided and corresponds to an abstract_network (VNet), it will create a NIC in that network’s first (or default) subnet. If no network given, it will use Azure’s default network (if one exists) or create a new VNet implicitly (with a default address space). It will also create a disk for the OS (with default size e.g. 30GB unless specified in annotations) and attach it. If no custom image is given, it will use a default image (e.g., the latest Ubuntu LTS image from Azure marketplace, or another common OS). Azure VMs typically need admin credentials; the provider might generate a random admin username/password or require the user to provide an admin_password or SSH key in annotations (if none provided, a random password could be set and output as a sensitive value). The NSG (firewall) will be configured to allow outbound traffic and no inbound by default (similar to AWS). In short, the provider will orchestrate the creation of all necessary pieces so that the user only sees one resource.
GCP: Creates a Compute Engine VM instance. If network_id is provided, the instance’s network interface will use that network (and default subnet in the specified region). If not, it uses the default VPC network of the project. The provider selects a base image if none specified – likely a Debian or Ubuntu image from GCP’s public images. A service account for the VM can be auto-created or the default compute service account is used by default. Firewall rules in GCP’s default network allow certain traffic like SSH by default; if using a custom network, the provider might also create a default firewall rule to allow outbound and maybe SSH inbound similar to other clouds (or instruct the user to handle firewall via separate resources if needed). The instance machine type is determined by the size or instance_type field (see schema below), mapping generic sizes to GCP machine families (e.g., "small" might map to e2-medium or similar). Boot disk is created automatically using the chosen image.
Schema Design: The abstract_instance schema provides common properties for a basic VM:
name – string, optional: Name of the instance. If not provided, a unique name will be generated. For AWS this sets the Name tag, for Azure this is the VM name, for GCP the instance name.
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location to deploy the instance. If omitted, the provider’s default region for that cloud is used. (Azure also needs a resource group – if not provided via annotation, the provider will use the default resource group from config or create one.)
image – string, optional: An OS image identifier or shorthand. This field is abstracted to avoid cloud-specific image IDs. The user can provide generic values like "ubuntu-20.04" or "windows-2019" and the provider will look up a corresponding image in the target cloud (e.g., latest Ubuntu 20.04 LTS AMI in AWS, or Azure marketplace image SKU, etc.). If image is not provided, a default (e.g., latest stable Ubuntu LTS Linux) is used. Alternatively, the user can specify cloud-native image details via annotations if needed (for example, an exact AMI ID for AWS).
size – string, optional: A generic machine size or tier (e.g., "small", "medium", "large", or numeric like "2vCPU-8GB"). The provider will map this to an actual instance type in each cloud. For instance, "small" might map to an AWS instance type like t3.small (2 vCPU, 2GB), an Azure VM size like Standard_B1ms, and a GCP machine type like e2-small. The mapping will be documented and aims to choose roughly equivalent sizes. If size is not given, a default small size is used (suitable for a micro test server). As an alternative, the user could bypass this by specifying an exact instance type in the annotations (e.g., instance_type = "m5.large" for AWS) if they want a specific class – but doing so ties the config to that cloud’s naming.
network_id – string, optional: An ID of an abstract_network resource to attach this instance to. If provided, the instance will be created in that network’s context (VPC/VNet). If omitted, uses the cloud’s default network environment (default VPC, etc.). On AWS, if a subnet ID is needed, the provider picks a subnet in the given VPC (if the abstract_network created subnets, use the first; or create one). On Azure, the provider uses the VNet’s default or first subnet. On GCP, if using a custom network, ensure a subnet exists in the region.
public_ip – bool, optional: Whether to assign a public IP to the instance. Default could be false for security unless explicitly true. If true, the provider will allocate or enable a public IP (Elastic IP on AWS associated with the instance, Azure will create a Public IP resource for the NIC, GCP attaches a public IP by default if network is default or if configured).
annotations – map(string->string), optional: Cloud-specific parameters. Examples:
AWS: instance_type (if user wants to directly specify), key_name for an existing SSH key, security_groups to attach custom security groups, etc.
Azure: resource_group (to specify RG), size (Azure VM size if wanting to override generic mapping), admin_username, admin_password or admin_ssh_key for login (if not provided, the provider may generate credentials).
GCP: machine_type (if overriding size mapping), subnet_name if a specific subnet in the network should be used, service_account or scopes for the VM’s service account permissions.
Output attributes: Each instance will output an id which is the cloud identifier (e.g., the EC2 Instance ID, the Azure VM resource ID, or the GCP instance self-link). It can also output private_ip and public_ip (if assigned) for the instance, which are useful for configuring other components. Possibly hostname or instance_name could be output as well. Sensitive outputs like an admin password (if auto-generated) would be marked sensitive and ideally provided via Terraform’s output mechanisms.
Behavior and Lifecycle: Stopping, starting, or reconfiguration of instances is limited. Changes to certain fields:
Changing image or size on an instance will likely replace the instance (since resizing or reimaging is complex cross-cloud). We will mark those fields as ForceNew for simplicity.
Minor changes like adding an annotation for a security group might be handled in-place if the cloud allows (e.g., update AWS security groups).
The provider will ensure that deletion of an abstract_instance also cleans up any associated resources it created (e.g., Azure NICs, disks, public IPs if they were auto-created, etc.) to avoid orphaned resources or leaks.
Example usage: (for illustration, not a separate section)
# Example: Launch an instance in each cloud using similar config
resource "abstract_instance" "aws_vm" {
  name    = "app-server"
  type    = "aws"
  size    = "small"
  image   = "ubuntu-20.04"
  network_id = abstract_network.vpc.id   # using an abstract network defined elsewhere
  public_ip  = true
  annotations = {
    key_name = "my-ssh-key"  # AWS-specific: use an existing key for SSH
  }
}
resource "abstract_instance" "azure_vm" {
  name    = "app-server"
  type    = "azure"
  size    = "small"
  image   = "ubuntu-20.04"
  network_id = abstract_network.vnet.id  # Azure VNet from abstract_network
  public_ip  = true
}
resource "abstract_instance" "gcp_vm" {
  name    = "app-server"
  type    = "gcp"
  size    = "small"
  image   = "ubuntu-20.04"
  # Using default network in GCP by omitting network_id
  public_ip  = true
}
The above would create three similar VMs on each cloud. In practice, you might not deploy all three at once, but it shows portability: aside from type, the config is nearly identical. Each will pick an appropriate instance type and image for that cloud.
Abstract Cluster
Description: An abstract_cluster resource represents a computing cluster or orchestrated cluster service. We define it as a managed Kubernetes cluster in this abstraction, since all three cloud providers offer Kubernetes as a common service (Amazon EKS, Azure AKS, Google GKE). By using Kubernetes as the abstraction for "cluster", we ensure the consumer gets a consistent interface (a Kubernetes cluster API) on any cloud
discuss.hashicorp.com
. The cluster includes the control plane managed by the cloud and a set of worker nodes. This resource essentially provisions a fully managed K8s cluster on the chosen provider with a specified number of worker nodes and basic settings. (Rationale: We choose Kubernetes because it’s a standardized cross-cloud interface for running containerized workloads. Alternatively, a simpler “cluster of VMs” could be considered, but that is effectively covered by using multiple Instance resources or an auto-scaling group, which is less standardized across clouds. Kubernetes offers a higher-level, uniform target for applications, aligning with the provider-agnostic goal.) Cloud mappings:
AWS (EKS): Creates an Amazon EKS cluster. This involves two main components: the EKS control plane (managed by AWS) and worker nodes (which can be managed node groups or self-managed autoscaling groups). The provider will handle creating the EKS cluster itself (specifying Kubernetes version if provided, otherwise default) and a default managed node group. The user can specify node_count and a generic node_size, which the provider maps to an EC2 instance type for the nodes (e.g., "medium" -> t3.medium or similar). It will create that many nodes (or an autoscaling group of that capacity). AWS EKS requires an IAM role for the cluster and node group; the provider will create these roles/policies under the hood (unless the user supplies them via annotations). It also requires subnets to deploy the cluster into – if the cluster network_id is provided (referring to an abstract_network/VPC), the cluster will use subnets from that VPC. If no network is provided, the provider may create a new VPC and subnets dedicated to the EKS cluster (or use the default VPC if available, though best practice is usually a dedicated VPC). The output of an EKS cluster will include the Kubernetes API endpoint URL and an authentication token or kubeconfig data so that the user can actually use kubectl with the cluster. The provider will retrieve the kubeconfig (certificate, endpoint, etc.) and make it available (possibly via an output attribute kube_config or by writing to a local file if configured). By default, AWS’s EKS control plane is highly available across AZs; the user can optionally specify annotations.region_zones to control which AZs or how many, but that might be beyond common denominator and thus not exposed directly.
Azure (AKS): Creates an Azure Kubernetes Service cluster. Similar to EKS, Azure’s managed Kubernetes needs a resource group, and optionally can integrate with other Azure resources. The provider will create an AKS managed cluster with default settings. It will ensure a node pool of node_count nodes using a VM size mapped from node_size. For example, "medium" might correspond to Standard_DS2_v2 instances for AKS nodes. Azure requires a few ancillary things: a service principal or managed identity for the AKS, and a Virtual Network. The provider can create a system-assigned managed identity for the AKS (Azure handles that if not provided) and will create or use a Virtual Network. If network_id is given (pointing to an abstract_network), the AKS cluster will be created in that VNet (and its nodes in a subnet). If not, the provider can either create a new VNet/subnet or use the Azure default infrastructure (Azure doesn’t automatically have a “default VNet” per se, so likely it will create one). The output for AKS will include the Kubernetes API server endpoint and credentials. Terraform’s AKS resource normally can output a kubeconfig; our provider will do similarly, outputting kube_config (or components like host, client certificate, token).
GCP (GKE): Creates a Google Kubernetes Engine cluster. This is straightforward as GKE is fully managed by Google. The provider will request a GKE cluster with the given number of nodes (node_count) and a machine type mapped from node_size (e.g., "medium" -> e2-medium nodes or similar). If network_id is provided, the cluster will be created in that VPC network (and in a new subnet or existing one in the specified region). If no network is specified, it uses the default VPC. GKE typically will create a default node pool if not specified; we will explicitly create one with the desired count and size. GKE outputs a cluster endpoint and an auto-generated kubeconfig as well (or one can be fetched with gcloud). The provider will capture the cluster’s endpoint, CA certificate, and generate an access token or use application default credentials to allow the user access. These will be provided as output attributes, similar to AWS and Azure.
Schema Design: The abstract_cluster focuses on high-level cluster settings:
name – string, optional: Name of the cluster. If omitted, a default name is generated. This will be used to name the cluster in the cloud (EKS cluster name, AKS resource name, GKE name).
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location for the cluster. If not set, uses provider default region for that cloud. (Note: Azure AKS requires a region and a resource group; the region can be deduced from resource group’s location if RG given. The provider can create a new RG in the specified region if needed.)
kubernetes_version – string, optional: Kubernetes version for the cluster (e.g., "1.26"). If not provided, the cloud will default to a current stable version. This field is common to all three (each lets you choose a version).
node_count – number, optional: Number of worker nodes in the cluster’s default node pool. Default might be 3 (a typical small cluster) if not specified. The provider will create this many nodes. If the cloud auto-scales or requires a minimum, we can set auto-scaling parameters via annotations (e.g., min/max size).
node_size – string, optional: Generic size for the worker nodes (similar to instance size). For example "medium" or "large". The provider maps this to an instance type in each cloud’s context (like it does for instances). If not provided, a default (e.g., medium) is used.
network_id – string, optional: Reference to an abstract_network if the cluster should be launched in a specific network/VPC. Highly recommended for AWS and Azure to control network placement. If not provided, the provider will implicitly handle networking: e.g., create a new VPC or VNet just for this cluster (with appropriate subnets and routing). On GCP, if not provided, uses default network.
annotations – map(string->string), optional: Cloud-specific optional settings:
AWS: could accept eks_role_arn if the user wants to supply an existing IAM role for the EKS cluster, or eks_node_role_arn for node IAM role (otherwise provider creates roles). Could also allow node_instance_type if user wants a specific instance beyond what node_size chooses. Another useful annotation might be fargate = true to create an EKS Fargate (serverless) profile instead of nodes, if that were in scope (but likely not for initial design since Azure/GCP analog would be AKS virtual nodes or GCP autopilot – those features might be skipped initially).
Azure: resource_group to specify an existing RG to create the AKS in. node_vm_size if specific. Possibly enable_auto_scaling, min_count, max_count if user wants cluster autoscaler (since Azure and GCP support it; AWS EKS node groups too). We might keep auto-scaling out of core and add later, but it's something common enough that could be added in future.
GCP: project (if wanting to override default project), enable_autopilot (if one wanted GKE Autopilot mode rather than standard, though that changes semantics of node_count), etc., or node_machine_type.
Output attributes: All clusters will output:
id: an identifier for the cluster (could be the ARN for EKS, resource ID for AKS, name or self-link for GKE).
kube_config: A sensitive output containing the kubeconfig credentials to connect to the cluster. This might be a raw kubeconfig file content or a map of connection data (host, CA cert, token). For example, abstract_cluster.my.id might not be very useful by itself, but abstract_cluster.my.kube_config could be used by automation to configure kubectl or deploy apps. We ensure this output is marked sensitive in Terraform state (since it contains credentials).
Possibly endpoint: the Kubernetes API server URL (for informational use), and cluster_ca_certificate or client_certificate depending on auth (some clouds give client certs, some use token).
node_count: (for awareness, the actual number created, which should match input unless autoscaled).
We might output node_ids or node_instance_ids (like EC2 instance IDs of workers, node pool names, etc., mostly for debugging or if needed to attach other resources). However, that introduces cloud-specific detail, so we might not expose node IDs in the generic interface.
Behavior and Notes: The provider will manage cluster creation and handle differences:
EKS can take ~10+ minutes to provision; the provider must poll until the cluster is ready and nodes are registered. Similarly, AKS and GKE creation is asynchronous. The Terraform apply will wait until the cluster is fully ready (or at least until credentials can be fetched).
Credentials handling: AWS EKS typically requires invoking AWS STS to get a token for kubeconfig (unless using the AWS IAM authenticator). We might leverage the AWS Terraform provider logic or AWS SDK to get a short-lived auth token, or create a Kubernetes config with exec authentication (which might complicate usage). Alternatively, we could create a Kubernetes service account in the cluster with admin and fetch its token – but that’s additional steps. For simplicity, initial version might just output instructions to use the AWS CLI to get credentials, but that’s not user-friendly. Ideally, we output a ready-to-use kubeconfig. Azure AKS and GCP GKE offer direct client certificate and kubeconfig data, so easier there. We will aim to provide a unified kubeconfig output for all.
Upgrades and changes: Changing node_count can be applied in place for AKS and GKE (and for EKS managed node groups) – we will attempt to update the node pool size if possible without recreating the cluster. Changing kubernetes_version might trigger an upgrade process (if supported by the cloud) or force recreation if not easily upgradable. Possibly mark it ForceNew to avoid complexities of orchestrating upgrades.
If a user wants to destroy an abstract_cluster, the provider will delete the cluster and all nodes. For AWS, that includes deleting the node group and possibly the CloudFormation stacks EKS uses behind scenes (the provider will call the appropriate APIs to delete cluster and node group). Any implicitly created VPC for the cluster should also be deleted if not used elsewhere (though Terraform won’t know dependencies outside it; we might choose to not auto-create a VPC and instead always require network or default, to avoid tricky deletion scenarios).
Multi-cloud considerations: Two cluster resources of different types (clouds) have no direct relationship, but a user could use them in a config if they want clusters in multiple clouds for redundancy. Our provider handles that concurrently.
Abstract Bucket
Description: An abstract_bucket provides cloud-agnostic object storage. This corresponds to an S3 bucket on AWS, a Blob Storage container on Azure, and a Cloud Storage bucket on GCP. Object storage is a key common service for files and blob data, and this abstraction ensures a uniform way to create buckets and configure basic options like versioning. Cloud mappings:
AWS (S3): Creates an Amazon S3 bucket with the specified name. S3 bucket names are globally unique, so the provider will directly use the name given. The region can be specified (S3 allows choosing a region for bucket data; if not provided, defaults to AWS provider region). Common S3 features like versioning or server-side encryption can be toggled – versioning is exposed as a top-level field since all providers support it. If versioning is enabled, the provider will call to enable bucket versioning. Other S3 settings like ACL (public/private) can be controlled by annotations (acl key). By default, the bucket will be private (no public read access) unless annotations like acl = "public-read" are set, in which case the provider will apply that ACL. The provider may also consider annotations.force_destroy to allow emptying and deleting a non-empty bucket on destroy (to mimic Terraform’s force_destroy parameter for S3/GCS). If not specified, the user is expected to clean up objects or accept that destroy might fail on non-empty buckets.
Azure (Blob Storage): In Azure, object storage is a bit different – blobs reside in containers within a Storage Account. To fit this model, the abstract_bucket will create an Azure Storage Account (if needed) and a Blob Container. We have a few design choices:
We can create one storage account per abstract_bucket resource, using the bucket’s name (or a derived name) as part of the account name. Storage account names have stricter rules (lowercase, 3-24 chars, unique globally). If the given bucket name is valid and <=24 chars, we might use it directly as the account name; if not, we can generate one (like take first 20 chars + random suffix). The container inside will also have a name (which could reuse the same name if it meets container naming rules, which are similar to bucket rules).
Alternatively, we could allow an annotation storage_account to either use an existing storage account or to name the one to create. If a user wants multiple containers in one account (for cost or management reasons), they might create one abstract_bucket as the account and others as containers in it – but that breaks the one-to-one abstraction. Simplicity suggests each abstract_bucket corresponds to one container and its own storage account, isolating them (at the cost of potentially more accounts). We'll choose the simpler route: each Azure bucket results in a new Storage Account + Container, unless the user explicitly wants to reuse an account (then they can provide the storage_account name in annotations and the provider will create only the container in that existing account).
Region: Azure storage accounts are created in a region (Azure calls it location). We use the region field for this.
Other settings: If versioning is enabled, Azure accomplishes that via "soft delete" for blob versions (Azure supports blob versioning as of 2020). The provider will enable blob versioning on the storage account if requested. For encryption, Azure storage accounts are encrypted by default. Public access: Azure by default prevents public blob access at the account level; if an annotation like public_access = true is given, the provider will enable container-level anonymous read access (container ACL "blob" or "container").
Azure requires a resource group for the storage account. The provider uses annotations.resource_group if provided; otherwise, it uses the provider’s default RG or creates one (similar to earlier patterns).
GCP (Cloud Storage): Creates a GCP Cloud Storage bucket. Bucket names in GCP are globally unique like AWS. The region can be multi-region (like "US") or single region (like "us-central1"); if region is provided, we create a regional bucket, otherwise maybe a multi-region default (like "US" multi-region). The provider will call the Storage API to create the bucket with given name and location. Versioning is supported and will be enabled if requested. Public access can be toggled via annotations (e.g., public = true could remove all IAM restrictions or add a "allUsers: objectViewer" permission). Encryption can also be handled via annotation if needed (e.g., use Google-managed encryption vs customer-managed key). By default, GCP buckets block public access unless configured.
Schema Design: The abstract_bucket schema is straightforward:
name – string, required: The name of the bucket. This must conform to DNS-style naming rules (lowercase, no spaces, etc.) because AWS and GCP require it. Ideally 3-63 characters. The provider will validate the name format. (If a name is not unique or invalid for a particular cloud, the error will come from the cloud API; we surface that to the user).
type – string, required: "aws", "azure", or "gcp".
region – string, optional: Region/Location for the bucket’s data storage. If omitted, a provider-specific default is used (e.g., for AWS, default region; for Azure, the default location from provider config; for GCP, a multi-region like "US" if not specified).
versioning – bool, optional: Enable versioning for the bucket. Default false. If true, the provider will turn on versioning (or the equivalent feature) for the bucket on all clouds.
annotations – map(string->string), optional: Cloud-specific extras:
AWS: acl (e.g., "private", "public-read"), force_destroy ("true"/"false" as string) to delete non-empty bucket on destroy, encryption (like "AES256" for S3 default encryption), tags for tagging.
Azure: resource_group to place the storage account, account_sku (e.g., "Standard_LRS" vs "Standard_GRS" for redundancy), public_access (e.g., "blob" to enable public read, or "none" to explicitly block public access – Azure uses a container-level setting).
GCP: storage_class (e.g., "STANDARD", "NEARLINE"), public (true/false to set allUsers read access), force_destroy (similar semantics to AWS, to auto-delete objects on bucket deletion).
Output attributes:
id: The bucket ID (for AWS/GCP this might just be the bucket name, since that uniquely identifies it; for Azure it could be the full resource ID of the storage account or a composite like <account>/<container>).
url: A URL or endpoint for the bucket. For example, an S3 bucket’s URL (e.g., https://my-bucket.s3.amazonaws.com), Azure blob container URL (https://<account>.blob.core.windows.net/<container>), or GCP bucket public URL (gs://bucketname or https://storage.googleapis.com/bucketname). Providing a URL helps users construct links or integrate across clouds (e.g., an Azure function might need the S3 bucket’s URL to fetch an object).
Possibly arn for AWS (S3 ARN), but that’s AWS-specific and usually not needed outside AWS. We likely omit cloud-specific identifiers from outputs unless necessary for interpolation with other resources. Instead, id can serve as an identifier to pass to other abstract resources (though currently none of the other abstract resources directly consume a bucket id, except maybe a function’s code source which we are not doing here).
Behavior:
Bucket creation is generally quick and synchronous on all clouds. The provider will verify creation and enable requested settings (versioning, ACLs).
Deletion: We consider force_destroy annotation to let the provider automatically delete all objects if the user wants Terraform to fully clean up. Otherwise, a bucket with objects will error on deletion (consistent with Terraform AWS/GCP provider unless force_destroy is true). We will document this to the user.
Immutability: Changing the name of a bucket or type will force new resource (you can't rename buckets in AWS/GCP easily, nor switch cloud obviously). Changing region would also recreate the bucket (data won't migrate).
Changing other settings like turning on versioning can be done in-place (the provider just enables versioning on the existing bucket).
The provider should handle Azure’s two-resource scenario carefully: when destroying an Azure bucket resource, it should delete the container and the storage account if it was created solely for this bucket. If multiple abstract_bucket resources share one storage account (via annotations), we must avoid deleting the account until the last container is gone. To simplify, we might discourage sharing and treat each abstract_bucket as owning its storage account unless explicitly told otherwise.
Abstract Function
Description: An abstract_function represents a serverless function deployed on a cloud’s Function-as-a-Service platform. This maps to AWS Lambda, Azure Functions, and Google Cloud Functions. It allows users to deploy code (typically small units of logic) without managing servers, in a cloud-agnostic way. The abstraction covers the function code, runtime, memory, and a basic trigger configuration. Cloud mappings:
AWS (Lambda): Creates an AWS Lambda function. The provider will package the given code (if it's provided as a local directory or file) into a ZIP if necessary and upload it to Lambda (either directly via AWS API or first to an S3 bucket if required for larger packages – the provider can handle small packages directly and fall back to S3 for larger). The runtime (e.g., Python 3.9, Node.js 14, etc.) is specified in a cloud-neutral way and mapped to AWS’s runtime identifiers (for example, "python3.9" maps to AWS runtime "python3.9", "nodejs14" -> "nodejs14.x" in AWS notation). The handler (entry point) needs to be specified for AWS (like "handler.function"). We might derive a default if not provided, or require it if the language needs it. Memory size and timeout are applied straightforwardly to Lambda. AWS Lambda also needs an IAM role for execution. The provider will create a default IAM role (with basic CloudWatch Logs permission) for the function unless one is provided via annotations (e.g., role_arn). If the user wants the function to be triggered by HTTP, AWS has Lambda Function URLs (a built-in HTTP endpoint) which can be enabled via an annotation or field trigger_http = true. If enabled, the provider will configure a Function URL and output the URL. (Alternatively, the user could connect an API Gateway, but that’s out of scope for this abstraction). We will expose the function’s ARN as id (or as a separate output), and possibly the function URL if enabled. Environment variables are supported and will be set as part of Lambda configuration.
Azure (Functions): Azure Functions are part of a Function App, which is essentially a container for one or more functions sharing resources. The provider will model each abstract_function as a separate Function App with a single function deployment, for simplicity. Specifically, for an Azure function, the provider will:
Ensure an Azure Storage Account exists (Azure Functions use it for handling triggers and logging). The provider can create a storage account per Function App (naming it after the function perhaps), or use a common one. Simplest approach: each function gets its own storage account (similar to how we did for buckets, to avoid name collisions and cross-usage complexities).
Create a Function App (in Consumption plan by default) in a given region and resource group. If annotations.resource_group is provided, use that; otherwise, use default RG or create one.
Deploy the code: if code is provided as a zip, we upload it to the function. If it’s source files, we may need to zip it first. Azure requires specifying a runtime stack for the Function App (like Node 14, Python 3.9, .NET, etc.), which corresponds to our runtime. The provider will map runtime strings to Azure Functions runtime settings (for example, "python3.9" -> use Python 3.9 runtime stack, "nodejs14" -> Node 14, etc.).
Azure Functions also require a name for the function and the Function App; we’ll use the name for both (with some normalization since function app name must be globally unique and <= 60 chars). The function’s entry point (function name) might be determined by code or a setting; we can require a handler field for consistency, but Azure often uses an index.js or run.ps1, etc. Possibly we just assume a convention or let the code package include the function definition as Azure expects (like a function.json). To keep it simple, we might require the user to provide a ready-to-run Azure Function package (with correct structure) if using Azure. This is an area where true abstraction is hard – runtimes and packaging differ. However, if our aim is to feed this to Codex, we might outline a simplified model: assume the code is a single function in a single file that each cloud can handle similarly (not fully realistic, but for design).
If trigger_http = true (if we choose to support triggers as a field), we will configure an HTTP trigger for the Azure function. By default, Azure Functions will treat a function named "HttpTrigger" with proper attributes as HTTP. We might instead ask the user to specify trigger type as part of configuration: e.g., trigger = "http" or trigger = "event". For now, support only "http" or none, since HTTP trigger is common to all. If HTTP trigger, the provider needs to set the function’s auth level to "anonymous" or "function" depending on if we want a public URL. We'll choose to allow public (anonymous) HTTP triggers if requested, and output the URL (which is typically https://<functionapp>.azurewebsites.net/api/<functionName>).
Environment variables are set via Application Settings in Azure Function App.
The provider will automatically create the needed Storage Account and Application Insights (if required – though App Insights is optional; Azure will create one by default for a new Function App unless disabled via an setting, we might disable it via annotation to reduce extraneous resources).
GCP (Cloud Functions): Creates a Google Cloud Function. GCP’s functions are quite straightforward: provide code (zip or source), specify runtime (e.g., "python39", "nodejs14"), entry point (function name), and trigger type. We will use Cloud Functions 1st Gen or 2nd Gen accordingly, but keep it simple. The provider will upload the code (the GCP API might allow direct upload or via Google Cloud Storage, which we handle internally possibly by using the GCP Go SDK to upload the zip). For runtime mapping, "python3.9" -> "python39", "nodejs14" -> "nodejs14", etc. Memory and timeout are set as specified. If trigger_http = true, we create an HTTP-triggered function (GCP supports that easily, just a flag). If no trigger specified (or if we wanted to support background triggers, that gets complicated, so default to HTTP or none), we can default to an HTTP trigger as it’s common. The function’s URL (for HTTP trigger) will be output (GCP gives a https endpoint). If not HTTP, the user could trigger it manually or via other GCP events (but those are cloud-specific; we likely only do HTTP for initial design).
GCP requires a project (from provider config) and region for the function. We use the region field for that (GCP Cloud Functions are regional).
The provider might need to enable the Cloud Functions API on the project if not already (which Terraform can’t directly do except via another resource; maybe assume it’s enabled or instruct user to enable beforehand).
Schema Design: The abstract_function schema centers on deploying code with config:
name – string, required: Name of the function. Will be used to name the function on each cloud (must meet naming rules: e.g., AWS allows [a-zA-Z0-9-_]+, Azure function app name must be unique DNS name, GCP function name has length and char limits). The provider may adjust or validate names accordingly (e.g., force lowercase for Azure DNS name).
type – string, required: "aws", "azure", "gcp".
region – string, optional: Region to deploy the function (AWS Lambda region, Azure Function region, GCP region). If omitted, uses default region from provider config for that cloud.
runtime – string, required: The runtime/language for the function. Examples: "nodejs14", "python3.9", "dotnet6", "go1.18". We will support a defined list of runtimes that all three clouds have some support for (Node.js, Python, perhaps Java). The provider will map this value to the exact identifiers:
AWS: directly corresponds in many cases (e.g., "python3.9" is an AWS Lambda runtime name).
Azure: we have to map to runtime stack settings (for example, Node 14 -> use NODE|14 setting in Application Settings, Python 3.9 -> PYTHON|3.9).
GCP: map to names like "nodejs14", "python39".
code – string, required: Path to the function code. This could be a local file or directory path. The provider will interpret it as follows:
If it’s a directory, it will zip the contents of that directory.
If it’s a zip file path, it will use it directly.
(In future, we might allow referring to an artifact in cloud storage, but for simplicity we assume local code packaged with Terraform.)
This is similar to how Terraform AWS lambda_function resource accepts filename. The provider then handles upload to the cloud.
handler – string, optional: The entry point of the function code. For interpreted languages, this is typically "file.function_name". For example, "index.handler" for Node or "main.my_func" for Python. AWS requires this field. Azure and GCP also need an entry point:
For Azure, if using a custom handler, we might require the function name (which would match a function in code or a configuration in function.json). This is tricky to abstract; but we could say if not provided, we assume the code package itself defines the function (Azure can detect a function via function.json).
For GCP, the handler is the name of the function in the code that will be invoked (e.g., HelloWorld).
We might make handler required for AWS/GCP and ignore it for Azure if Azure’s package structure self-describes the function (or also use it to generate a function stub).
memory – number, optional: Memory in MB for the function. Defaults to a baseline (e.g., 128 MB). All three providers allow setting memory. (Azure Functions on Consumption plan allocate resources differently, but one can set a memory limit in premium plans – however, we can assume 128MB triggers some default. Azure consumption plan might not allow manual memory setting; in that case, we may ignore or document that Azure will auto-scale the memory up to some limit).
timeout – number, optional: Timeout in seconds for function execution. Defaults to, say, 30 seconds. AWS supports up to 15 minutes, Azure default is 5 minutes (can be increased), GCP default 60s (max 540s for 1st gen). We'll allow up to the lowest common maximum, or just trust user to keep it reasonable. Setting it will apply in each (we’ll clamp to cloud max if exceeded).
environment – map(string->string), optional: Environment variables to set for the function. Each will be passed to the function at runtime. All three clouds support environment variables for functions. (We must be careful to store them as sensitive if they contain secrets – Terraform allows marking sensitive, which we should do if any value looks like a secret, but more simply we might treat all env as non-sensitive in state unless the user uses a sensitive var).
trigger_http – bool, optional: (Tentative common field) If true, the function will be deployed with an HTTP trigger/public endpoint. Default false. When enabled:
AWS: enable Lambda Function URL (which gives a URL like https://<id>.lambda-url.<region>.on.aws).
Azure: configure the function with an HTTP trigger (anonymous auth), resulting in a URL for the function.
GCP: deploy as an HTTP-triggered function (which automatically creates an HTTPS endpoint).
If false, the function is deployed with no public trigger (it can still be invoked by other cloud-specific means or manually). Security: The HTTP endpoints will be publicly accessible unless additional auth is required (not in scope for our abstraction – we assume if you set trigger_http, you want a public endpoint for simplicity).
annotations – map(string->string), optional: Cloud-specific options:
AWS: role_arn if using an existing IAM role for the Lambda (if not provided, provider creates a basic execution role), runtime_version (for finer control if needed), subnet_ids and security_group_ids if the Lambda should be in a VPC (not in all clouds, but if one wants Lambda in VPC to access DB, etc., they could specify those).
Azure: resource_group (for the Function App), plan_sku if wanting to specify a different hosting plan (Consumption vs Premium), disable_app_insights to not create Application Insights.
GCP: project (override), allow_unauthenticated (true/false, for HTTP trigger whether it’s open to public; by default we’d set true if trigger_http true, but user could require auth), vpc_connector if function should access a VPC.
Also, if in future we support non-HTTP triggers, annotations might carry things like event trigger specification (but out of initial scope).
Output attributes:
id: The function’s identifier (could be the ARN for Lambda, the Function App name or combined ID for Azure, and the name or full path for GCP).
invoke_url: If trigger_http was true, this will be the publicly accessible URL to invoke the function (for AWS, the Lambda function URL; for Azure, the function’s URL; for GCP, the HTTPS endpoint). If trigger_http is false, this may be empty or not present.
Perhaps invoke_arn for AWS if needed, but usually not required externally.
Possibly outputs to indicate the runtime or memory as deployed (mostly echoing inputs).
We will ensure not to expose sensitive info. If a function needs an API key (Azure functions have function keys for non-anonymous triggers), those would be output only if relevant and marked sensitive.
Behavior:
Code Packaging: The provider will likely need to package and upload code. This can be done using Go libraries to zip files and then using each cloud’s API. We must document that the code path should point to the correct content. Large packages might require special handling: e.g., AWS Lambda if >50MB requires S3 upload – our provider could transparently use an AWS S3 bucket (maybe the abstract_bucket resource or a temp bucket) to store the code and then point Lambda to it. But that’s an edge case; initially, we can set a reasonable limit and throw an error if exceeded.
Deploy & update: If the code changes (by checksum), Terraform will update the function in place (all providers support updating code without recreating the function). If certain configs change like runtime, some providers might require recreation or may support in-place update. We will attempt in-place updates for code, env, memory, timeout. Changing name or type obviously recreates on a different cloud. Possibly changing trigger_http from false to true can be done in place (enable a trigger) – we’ll handle enabling/disabling accordingly.
Concurrency and scaling settings (like AWS Lambda concurrency limits, GCP min/max instances, Azure plan capacity) are not exposed in the first version, as they are advanced. The defaults (auto-scale on demand) will suffice.
Dependencies: If a function needs to interact with other resources, it can reference them through Terraform as usual. For example, a function might use a bucket name from an abstract_bucket (as environment variable) to know where to fetch/store data. This is fully supported since the provider yields those values. Another case: if a function should be in the same network as an abstract_database (to access it without public internet), that’s tricky in AWS (Lambda in VPC) or Azure (Function in VNet). We may allow the user to provide VPC info via annotations for AWS and Azure (e.g., subnet_ids for Lambda, or use Azure Functions Premium Plan with VNet integration). But since network integration for serverless is a bit advanced, we can leave it for future extension. Initially, functions can be assumed to run with public access to the internet (thus they can reach any public endpoint including cross-cloud endpoints if necessary).
Abstract Database
Description: An abstract_database resource provides a managed relational database instance. It corresponds to AWS RDS (specifically a DB instance for a chosen engine), Azure Database for MySQL/PostgreSQL (or Azure SQL), and Google Cloud SQL. This abstraction lets users set up a basic SQL database (e.g., MySQL or PostgreSQL) in a cloud-agnostic way, covering the common parameters like engine type, version, instance size, and credentials. Cloud mappings:
AWS (RDS): Creates an RDS DB instance. We will support open-source engines that exist in all clouds: MySQL and PostgreSQL (possibly also MariaDB as it's in RDS and Azure, but GCP dropped MariaDB support in Cloud SQL? Actually GCP supports MySQL, Postgres, and SQL Server. MariaDB not sure. For simplicity, stick to MySQL and PostgreSQL). The user provides engine and optionally version. The provider will choose a DB instance class based on a generic size (like we do for VMs). If not specified, default to a small instance type (db.t3.small or similar). Storage size default perhaps 20 GB. Multi-AZ can be toggled via an annotation multi_az = true if desired (common enough to perhaps include as a top-level field, but since Azure Single Server vs Flexible with HA differ, we keep it as annotation). The DB will be created in a subnet group – if an abstract_network is provided (via a network_id field or similar), the provider will ensure the RDS is in that VPC. If network not given, it will use default VPC subnets. By default, the DB is not publicly accessible (we can default to private for security; if user wants public, an annotation like public = true can override to create with PubliclyAccessible true and set up security group to allow some ingress). The provider will set a master username and password. We may require the user to provide a password (to avoid storing it in state plain). Possibly accept password = var.db_password (marked sensitive) in the config. If not provided, generate a random password and output it as a sensitive value (the user then can retrieve it via Terraform output). The endpoint (hostname and port) will be output for connectivity. RDS also requires minor things like parameter groups; the provider will use the default parameter group unless advanced tuning is needed (out of scope).
Azure (Database for PostgreSQL/MySQL): Azure has two offerings: the older Single Server (which was for MySQL/Postgres) and the newer Flexible Server, as well as Azure SQL for MS SQL. Since we want cross-cloud, focusing on MySQL/Postgres, we can choose Flexible Server (which is the current recommended for MySQL/Postgres on Azure). The provider will create an Azure Database for MySQL or PostgreSQL flexible server in the given region. It also needs a resource group and a Virtual Network if using VNet integration (optional). If network_id is given, we can enable VNet integration and deploy the DB into that VNet (Azure flexible server can be deployed with private access in a subnet). If no network, we can default to public access (which means Azure will create it with a public endpoint, protected by firewall rules). By default, to align with others (which default to private), we might create it with no public access, but then how to access it? Possibly better to allow public by default but locked down by firewall to the client IP or no access until configured. This is tricky; perhaps simpler: if no network given, Azure DB will be public but the provider will set the firewall to allow no IPs (essentially it's inaccessible until user opens). That way it's secure by default. The provider sets admin username and password similar to AWS. Azure requires the username to have a domain in it (like adminuser@servername in connection strings), but we can hide that detail. The engine and version are specified (Azure supports specific versions like MySQL 8.0, PostgreSQL 14, etc.). We will map a generic engine and version to the Azure offering; if exact version not supported, we choose the closest or let Azure default. Sizing in Azure flexible server: you choose a SKU name or core count/memory; we'll map our size input to an SKU (for example, "small" -> 1 vCPU, 2GB). Storage size default maybe 20GB as well.
GCP (Cloud SQL): Creates a Cloud SQL instance for MySQL or PostgreSQL. The provider will specify database version (like "MYSQL_8_0" or "POSTGRES_14") based on engine and version input. Machine type is determined by a combination of tier or custom settings; we map size to either a predefined instance class (like db-f1-micro for smallest, db-n1-standard-1 for small, etc.) or use the newer custom vCPU+RAM settings. Perhaps simplest: "small" -> use a lightweight shared core instance (f1-micro or db-custom-1-3840 for 1 vCPU, 3.75GB?), "medium" -> 2vCPU, "large" -> 4vCPU, etc. The provider sets the user password as given or generates one. Networking: if network_id is provided, we will attempt to connect the Cloud SQL instance to that VPC (via a private IP). In GCP, that means setting up a VPC peering or using a Serverless VPC connector for Cloud SQL? Actually Cloud SQL private IP requires enabling a connection to a VPC (and selecting a subnet). We may need to ensure the provided network has an associated automatic subnet for the region or let Cloud SQL handle it. If no network is given, Cloud SQL by default can have a public IP. We likely default to public but require authorized networks to connect; by default we can leave it locked (no authorized networks). So similar approach: default to no external access unless user opens it. The endpoint (public IP or private IP) will be output.
Schema Design: The abstract_database captures core DB settings:
name – string, optional: Name of the database instance. If not provided, a name is generated. On AWS and Azure, the instance name becomes part of DNS endpoint (AWS uses random endpoint by default, Azure uses server name as DNS name, GCP uses instance name in connection string). We can set the name or let cloud generate one. We’ll use name if given for consistency (must be unique within cloud project/region).
type – string, required: "aws", "azure", or "gcp".
engine – string, required: Database engine type, e.g. "mysql" or "postgresql" (case-insensitive). Possibly allow "postgres" as alias for "postgresql". For extensibility, one might allow "mssql" (SQL Server) in the future if needed, but since not all clouds support it (AWS and GCP do, Azure has its own "Azure SQL" which is similar but not the same API), we can start with MySQL and Postgres.
version – string, optional: Version of the database engine, e.g. "14" for PostgreSQL 14, "8.0" for MySQL 8.0. If not specified, each cloud will default to a current major version. If specified, the provider picks the closest matching available version on that cloud. Minor version differences (like AWS might have 14.5 vs Azure 14.4) are abstracted.
size – string, optional: Generic size tier for the DB instance. Similar concept as instances: e.g., "small", "medium", "large", or numeric hints. This affects the CPU/memory of the DB. The provider maps it to:
AWS instance classes (db.t3.small for small, db.t3.medium for medium, etc., or if larger needed maybe db.m5.large for large).
Azure compute tiers (for flexible server, e.g., 1 vCPU Gen5 for small, 2 vCPU for medium).
GCP machine types (shared core vs standard).
If not provided, default to a small instance (like single CPU with 1-2GB).
storage_gb – number, optional: Storage allocation in GB. Default maybe 20 GB. User can increase this if needed. All clouds allow storage specification (and auto-growth in some cases, but we won't handle auto-grow settings in first pass).
username – string, optional: Master/administrator username for the database. Defaults to a standard name (e.g., "admin" or "root" depending on engine). If provided, must meet the requirements (like no reserved names, etc. – provider will validate).
password – string, optional: Password for the admin user. If not provided, the provider will generate a random strong password (and mark it sensitive). If provided in config, it should be via a sensitive input variable ideally.
network_id – string, optional: Reference to an abstract_network to deploy this database in a private network. If provided:
AWS: The RDS instance will be created in that VPC (the provider will create a subnet group covering the subnets of the given VPC, if not existing, and use that with publicly_accessible = false).
Azure: The DB will be created with VNet integration in that network’s subnet (the user might need to specify which subnet via annotations if multiple, or we pick the first). This implies no public endpoint.
GCP: The Cloud SQL will be set to private IP and linked to that VPC (via a connection).
If network_id is omitted:
AWS: RDS will default to the default VPC, and we could decide publicly_accessible = false (meaning only accessible within default VPC). To allow user to connect from outside (like from their PC), they'd need to either use a VPN/SSH or change it. Since that's advanced, leaving it private is safer. Alternatively, we might default to publicly_accessible = true if no network provided, so it's reachable over the internet (with proper security group rules). But security-wise, not ideal. Perhaps lean towards not public by default on AWS as well.
Azure: If no network, we create a public endpoint DB. Azure requires firewall rules; by default, we don't allow any IP. The user would have to open to their IP via Azure Portal or in future via an annotation.
GCP: If no network, we enable a public IP but with no authorized networks (so only accessible if user manually configures Cloud SQL Proxy or adds network rules).
annotations – map(string->string), optional: Cloud-specific settings:
AWS: instance_class to explicitly set a DB instance class (overrides size), multi_az ("true"/"false") to enable Multi-AZ deployment for high availability, public to override default public accessibility (e.g., "true" to force a public endpoint if desired), parameter_group_name if the user wants a custom parameter group, etc.
Azure: resource_group for the server, sku_name if wanting a specific SKU name, high_availability to enable zone-redundant HA (Azure Flexible Server has an HA option), public (true/false to allow public endpoint if in VNet scenario perhaps).
GCP: tier (like "db-f1-micro") to directly set machine type, availability_type ("ZONAL"/"REGIONAL" for HA), public (to explicitly enable/disable public IP).
We could also allow backup_retention_days, backup_window, but those might be too detailed for initial scope. Possibly skip.
Output attributes:
id: The database instance identifier (AWS RDS instance ARN or name, Azure resource ID, GCP instance connection name).
endpoint: The connection endpoint. Could be a sub-object with host and port, or a formatted "host:port" string. We will provide at least the host (endpoint DNS name or IP) and port (usually 3306 for MySQL, 5432 for Postgres – we know based on engine).
username: The master username (for convenience, if needed; not sensitive).
password: We will not output the password unless we generate it and want to provide it – but safer approach: if user didn't supply a password, we might output the generated one as a sensitive value. However, storing DB password in state is not great. We might instead encourage user to provide it. If we do generate, we could output it as sensitive so they can retrieve and then perhaps rotate manually. We'll document this carefully.
Possibly database_name: If we created a default database (some systems let you specify an initial DB name to create; we didn’t include that, but could via annotation).
Behavior:
Creation time can be several minutes on all clouds (especially if HA). The provider will wait until the instance is available, and then output the endpoint.
Password handling: We'll use Terraform's sensitive features for any password. For example, if generating, we might mark the internal state with the password (encrypted in state if user uses state encryption).
If a user loses the password, they can reset it by changing the password field (which the provider will apply via Modify commands, possibly with a short downtime for AWS/Azure).
Changing engine or major version would force new resource (no automatic engine migration).
Scaling up (size, storage): these can often be done in place (AWS allows instance class change with a reboot, Azure flexible can scale cores, GCP can scale machine type). We might allow increasing size or storage_gb in place, but for simplicity of Terraform lifecycle, we might also force new on size changes in the first iteration to avoid complex wait logic. Perhaps better: storage can increase in place (since all providers support that fairly straightforwardly), instance class can also change in place (with a reboot) – but implementing that requires extra API calls and wait. To keep provider logic lighter initially, we might mark size as ForceNew too. (This is a decision where we trade some efficiency for simplicity; could be revisited in future version).
Deletion: The provider will delete the DB instance. If the user didn’t take a snapshot, data will be lost. Possibly allow an annotation like skip_final_snapshot = true or final_snapshot_identifier = "name" similar to Terraform AWS provider. By default, to avoid surprise data loss, AWS RDS tries to snapshot on delete. We may by default skip snapshot for quick deletion (since this is an abstraction, or maybe do it?). For now, perhaps skip to avoid leaving behind resources. We can document that it's deleting outright.
The provider will ensure that if it created any sub-resources, they are cleaned (like AWS subnet group, Azure if any specific network resource, but likely not needed aside from the DB itself).
Multi-cloud usage: Not much interplay except maybe an app in one cloud connecting to a DB in another via the endpoint. The abstraction ensures each DB can be referenced for its endpoint and credentials as needed.
Multi-Cloud Deployment and Interoperability
One of the core benefits of the Abstract Cloud Provider is the ability to mix and match cloud resources in a single Terraform configuration – so-called hybrid deployments. Terraform inherently supports multiple providers in one config, and here we leverage that to allow a single provider (with internal multi-cloud capabilities) to provision resources across AWS, Azure, and GCP seamlessly. Single State, Multiple Clouds: All abstract resources, regardless of target cloud, are managed under the umbrella of the abstract provider. This means Terraform retains one state file that contains all these resources. Users do not need to initialize or maintain separate providers for each cloud (though behind the scenes the abstract provider itself handles multiple APIs). During a plan/apply, Terraform will orchestrate creation and destruction in the correct order respecting dependencies. For example, you could have an AWS bucket and an Azure function in the same config; if the function’s configuration depends on the bucket (say, using the bucket’s name in an environment variable), Terraform will create the bucket first then the function, even though they are different clouds, because the resource graph knows the dependency. Cross-Resource References: Because outputs from any abstract resource can be used as inputs to any other, users can wire components across clouds. For instance, an Azure function could read from or write to an AWS S3 bucket by using the bucket’s URL or name, which can be passed as an environment variable. In Terraform configuration, this looks like normal interpolation syntax:
resource "abstract_bucket" "data" {
  type = "aws"
  name = "my-data-bucket"
  versioning = true
}

resource "abstract_function" "processor" {
  type    = "azure"
  name    = "data-processor"
  runtime = "nodejs14"
  code    = "function_code.zip"
  environment = {
    BUCKET_NAME = abstract_bucket.data.name   # cross-cloud reference
  }
  trigger_http = true
}
In the above, abstract_bucket.data.name (which would be "my-data-bucket") is passed into the Azure Function's environment. The function code (running on Azure) can then use that value to call the AWS bucket (assuming appropriate access credentials are handled in code or via some IAM trust). The Terraform provider’s job is simply to ensure the data is flowed correctly. There is no inherent restriction – you could reference the output of an AWS resource in a GCP resource or any combination. The only caveat is that these references typically are just identifiers or endpoints; actual connectivity or permissions between the resources is outside Terraform's scope (e.g., to let an Azure function access an AWS bucket, you might need to provide AWS credentials to it or make the bucket public, which is a design choice left to the user). Provider Configuration for Multi-Cloud: As described earlier, the user needs to configure credentials for each cloud that will be used. They can choose to configure only AWS, for example, if no Azure or GCP resources are in the config. The provider will error at plan time if a resource is defined for a cloud that has not been configured. It's also possible to target multiple accounts or subscriptions for the same cloud by using multiple provider instances with aliases (like provider "abstract" { alias = "aws2" ... } with a different AWS credential), but that complicates usage and isn't a primary goal. Typically, one abstract provider instance corresponds to one set of AWS/Azure/GCP credentials at a time. Ordering and Dependencies: Internally, the provider does not need special handling for cross-cloud ordering – Terraform’s core engine sees all resources and their references. For example, if an abstract_database on GCP is needed before an abstract_function on AWS (maybe the function takes the DB endpoint), Terraform plan will establish that dependency and apply in correct order. The provider ensures that any attribute that might be needed is correctly exposed. We must ensure outputs that are needed across clouds are of compatible types (strings, etc.). For instance, an AWS bucket name is a plain string which can be passed anywhere. A more complex example: if an AWS Lambda needed to be triggered by a GCP pub/sub (not likely in our abstraction because triggers are not cross-cloud), that would be complex – but since we focus on loosely coupled components via data (names, URLs), we avoid entangling two cloud resources in one resource’s API. Handling Cloud-Specific IDs: When passing references, sometimes an ID is only meaningful in the context of that cloud (like an AWS VPC id has no meaning in Azure). We anticipate that and generally one would not pass such IDs across clouds unless it’s something like a network that exists in both (which doesn’t happen; you can't attach an Azure VM to an AWS VPC, as a forum post noted
discuss.hashicorp.com
). Our abstraction avoids that scenario by design (for example, each instance/cluster references a network of the same cloud). So, cross-cloud references will typically be things like strings or URLs that are just data. It's up to the user to use them appropriately (the provider isn't validating usage beyond Terraform’s normal type checking). Unified Logging/Outputs: Each resource will log its creation in Terraform separately. There isn’t a combined cross-cloud orchestration layer besides Terraform itself. Users will see, for example, an AWS API call trace for the bucket and an Azure API call trace for the function during terraform apply. The provider may group some actions (like for Azure function it creates storage account then function app, etc., but it will treat it as part of one Terraform resource). From the user's perspective, they just see one plan with all resources. State and Backends: Because multi-cloud deployments will often be used by teams to manage global infrastructure, the state file can become sensitive (containing all cloud resources). Storing state in a remote backend (like Terraform Cloud or an encrypted storage) is recommended, especially since it may include sensitive outputs (like DB passwords or function secrets). This is not different from normal Terraform usage, just a reminder that because multiple clouds are managed together, losing the state or having it corrupted could affect resources in all clouds at once. Example – Hybrid Deployment Scenario: Below is a sample Terraform configuration that showcases a hybrid deployment using the Abstract Cloud Provider. It sets up an AWS S3 bucket and an Azure Function that processes data from that bucket, as well as a GCP MySQL database that the function writes to. This contrived example demonstrates multi-cloud coordination:
provider "abstract" {
  aws = {
    region     = "us-east-1"
    access_key = "AKIA...YOURKEY"
    secret_key = "YOURSECRET"
  }
  azure = {
    subscription_id = "xxxx-xxxx-xxxx-xxxx"
    client_id       = "azure-client-id"
    client_secret   = "azure-secret"
    tenant_id       = "azure-tenant-id"
    location        = "eastus"
  }
  gcp = {
    project    = "my-gcp-project"
    region     = "us-central1"
    credentials = file("gcp-key.json")
  }
}

# 1. AWS S3 Bucket for data storage
resource "abstract_bucket" "data_bucket" {
  type       = "aws"
  name       = "myapp-data-bucket-123"   # must be globally unique
  versioning = true
  # AWS-specific: make bucket private (default behavior). Ensure we can delete it if not empty:
  annotations = {
    force_destroy = "true"
  }
}

# 2. GCP MySQL Database for persistent storage
resource "abstract_database" "app_db" {
  type     = "gcp"
  engine   = "mysql"
  version  = "8.0"
  size     = "small"
  storage_gb = 20
  username = "dbuser"
  password = var.db_password  # sensitive input variable
  # No network_id -> will be accessible via public IP (but no authorized networks by default)
}

# 3. Azure Function that processes data from S3 and stores results in the DB
resource "abstract_function" "processor" {
  type        = "azure"
  name        = "data-processor-fn"
  runtime     = "python3.9"
  code        = "${path.module}/function_code.zip"  # Package of function code
  handler     = "process.handle"        # e.g., in code, use process.py with function handle()
  memory      = 256
  timeout     = 30
  trigger_http = true                   # expose HTTP endpoint
  environment = {
    "BUCKET_NAME"   = abstract_bucket.data_bucket.name        # S3 bucket name (for AWS SDK usage in code)
    "DB_HOST"       = abstract_database.app_db.endpoint.host  # Database host from GCP
    "DB_PORT"       = tostring(abstract_database.app_db.endpoint.port)
    "DB_NAME"       = "appdata"
    "DB_USER"       = abstract_database.app_db.username
    "DB_PASSWORD"   = var.db_password                         # pass through same DB password (could also use abstract_database.app_db.password if output)
  }
  annotations = {
    resource_group = "rg-serverless"    # place function in this RG
    # The Azure function code is assumed to read BUCKET_NAME and connect to AWS using some credentials.
    # Those credentials could be set via environment or managed via Azure Key Vault (not shown here).
  }
}
Expected Behavior: When the user runs terraform apply on this configuration, the provider will perform the following steps (high-level):
AWS S3 Bucket – Create a new S3 bucket named "myapp-data-bucket-123" in us-east-1. Enable versioning on it. Since force_destroy was true, the provider will mark the bucket to allow deletion even if objects exist (by auto-emptying it on destroy). The bucket’s ARN and name are saved in state, and its name is output for use by others.
GCP MySQL Database – Create a Cloud SQL instance with MySQL 8.0 in us-central1. It will have a name like "app-db" (perhaps derived from abstract_database.app_db.name or generated). No VPC is specified, so it gets a public IP but no authorized networks – meaning it's reachable if you have the correct user/password and either using the Cloud SQL Proxy or you add network auth. Terraform will output the instance’s connection endpoint (something like app-db.us-central1.cloudsql.googleapis.com and port 3306) and credentials. The password is from var.db_password in this case, so known to the user, and not stored in plaintext in state (Terraform will mark the variable sensitive and not log it).
Azure Function – The provider will:
Ensure the resource group "rg-serverless" exists (if not, it might implicitly create it or throw an error if Azure APIs require explicit creation – ideally, the user would create an abstract_network or some Azure resource group abstraction, but for simplicity we allow the provider to auto-create the RG if needed).
Create a Storage Account for the function (e.g., named something like funcdpp5abcd since Azure needs one for function app).
Create a Function App (with a new Consumption Plan) named "data-processor-fn". It will set it to Python 3.9 runtime.
Deploy the code from function_code.zip to that Function App. The handler = "process.handle" might translate to Azure's function entry (Azure might use a different mechanism, but we'll assume this gets handled either by a function.json or by convention).
Configure Application Settings for the environment variables: BUCKET_NAME, DB_HOST, etc., injecting the S3 bucket name and DB endpoint that were created earlier.
Because trigger_http is true, configure an HTTP trigger for the function. This might mean Azure will have a function named "data-processor-fn" with an HTTP trigger with anonymous access. The provider ensures the function URL (something like https://data-processor-fn.azurewebsites.net/api/data-processor-fn) is retrieved.
Output the invoke_url for the function.
The function code, when executed (not part of Terraform’s concern), would use the BUCKET_NAME to fetch data from S3 (assuming it has AWS credentials, which could be set via environment or implicitly if using public bucket), then use DB_HOST/DB_USER etc. to connect to the MySQL DB and store results. This is beyond Terraform, but our provisioning made it possible to configure such integration.
Terraform finishes, reporting success. It would list the resources created: one AWS bucket, one GCP database, one Azure function. The Azure function’s URL might be output, which the user could then hit to trigger processing.
This scenario demonstrates that our provider can facilitate a multi-cloud application: data in AWS, compute in Azure, database in GCP – all defined in one cohesive config. The interpolation of values (like passing bucket name and DB host) is handled naturally by Terraform. The provider didn’t need special logic except providing those output fields. Multi-Cloud Differences: Users should be aware that not everything is perfectly uniform – for example, the Azure function’s cold start and scaling behavior might differ from, say, an AWS Lambda if we had one. But those details are at the cloud service level, beyond the abstraction. We focus on making sure the provisioning and wiring of resources is smooth.
Extensibility and Future Improvements
Designing an abstract provider involves careful consideration of scope. This section outlines how the provider can be extended in the future, and constraints to keep in mind: Adding New Resource Types: In the future, we may expand to other common services as long as they fit the lowest common denominator model. For example:
Queues/Topics: A messaging service abstraction (AWS SQS, Azure Storage Queues or Service Bus, GCP Pub/Sub). These are available in all three clouds, though with some differences. We could introduce an abstract_queue resource mapping to those. The schema might include attributes like FIFO vs standard (AWS has FIFO, others might not) so we’d stick to basics.
Load Balancer: All clouds have load balancers. We could abstract a simple L4/L7 load balancer that points to instances or functions. However, this might require also abstracting the target attachment, which gets complex.
Container Registry: All have container registries (ECR, ACR, GCR). A generic abstract_registry could manage a private image registry.
DNS Zones/Records: As noted in HashiCorp docs, DNS is fairly portable
developer.hashicorp.com
. An abstract_dns_record resource could be feasible to map to Route53, Azure DNS, Cloud DNS.
Secrets Management: Perhaps a stretch, but all three have secrets vaults (AWS Secrets Manager, Azure Key Vault, GCP Secret Manager). A unified abstract_secret might be possible.
When adding a resource, the steps would be:
Confirm the service exists in AWS, Azure, GCP in a comparable form.
Define a minimal common schema that covers the essential functionality.
Implement create/read/update/delete for each cloud’s API.
Document any annotations for edge cases or extra features.
We would maintain the design principle that if a resource’s concept doesn’t cleanly map to one of the clouds, we exclude it to avoid a poor abstraction. For example, serverless container services (AWS Fargate on ECS, Azure Container Instances, GCP Cloud Run) might be considered for abstraction since they are similar – that could be a future addition if demand exists. Supporting Additional Cloud Providers: While AWS, Azure, and GCP cover the majority of use cases, the provider could potentially support more clouds (with the understanding that "abstract" might lose meaning if it becomes too broad). If we were to add, say, Oracle Cloud or IBM Cloud in the future:
The provider type field could accept new values like "oci", "ibm", etc.
We would implement the same resource types for those clouds as much as possible. For example, if adding Oracle OCI: it has buckets (Object Storage), instances (Compute VM), maybe an equivalent of functions (Oracle Functions), and a managed database (Autonomous DB or MySQL). We’d map accordingly or decide if any resource isn’t supported on that cloud (then the provider should reject it if used with that type).
The provider configuration would need to accept credentials for the new cloud (e.g., an oci {} block with necessary keys).
We must ensure not to break existing users; adding new type options is backward-compatible, but we need to test that adding new cloud logic doesn’t interfere with others.
Modular Codebase: Internally, to ease future extensions, the provider code should be organized by cloud and resource. For example, we might have packages or files like aws_bucket.go, azure_bucket.go, gcp_bucket.go implementing each variant’s API calls, with a common interface so that the resource code simply calls CreateBucket(ctx, data, meta) and the implementation dispatches to the correct cloud-specific function. This modular approach allows contributors knowledgeable in one cloud to add improvements or new features for that cloud’s handling without affecting others. Annotations vs Schema Evolution: We anticipate that over time, some features currently hidden behind annotations might become mainstream enough across all clouds to promote to top-level schema fields. For instance, if all three clouds introduce a similar concept or if we initially omitted something like auto-scaling configuration for clusters and later find a uniform way to support it, we can add a new field (say enable_autoscaling, with subfields for min/max nodes) to the abstract_cluster resource. When doing so:
We maintain backward compatibility: existing configs using annotations would still work; the provider would just prefer the new first-class field if set.
We update documentation and encourage users to use the new field for clarity.
Backward Compatibility and Versioning: Changes to the provider will follow semantic versioning. Because it’s likely to be used in production IaC, we will be cautious with breaking changes. For example, if we realize a default we chose is not ideal, we may have to keep it for existing behavior and perhaps introduce a new option to change it. We will leverage Terraform’s Deprecated flags for schema if we ever need to rename or phase out a field or annotation key. The design from the start aims to minimize such churn by choosing clear names and patterns. Handling Provider Updates: We must keep track of cloud provider API changes. For instance, new AWS regions or Azure introduces a new runtime or deprecates an old one. The provider should periodically update the mappings (like available runtime values, instance types mapping, etc.). We could implement some of these as data files or small generated code to ease updates. Also, if any of the three providers starts supporting a feature that was missing (making it now a common denominator), we could move that feature from an annotation to a core field in a new version. Performance Considerations: With the provider orchestrating potentially many API calls (especially for complex resources like clusters or functions), we need to consider timeouts and parallelism:
The Terraform plugin SDK typically handles each resource create in its own routine. Our provider might be hitting three different APIs concurrently if resources are created in parallel. This is usually fine, but we should ensure thread-safety for any shared client objects. Likely we will not share clients across resources without locks, or we will create separate clients per resource invocation.
Long-running operations (like EKS cluster creation) might need increased timeouts. We will set appropriate timeouts in the provider (Terraform allows configuring a timeout on operations; we can set default and allow override via schema if needed).
We might implement some simple retry logic for transient errors on API calls to improve robustness across cloud calls.
Security and Credentials: Future improvements could include tighter integration with each cloud’s auth options:
For example, allowing the AWS credentials to be picked up from the environment or EC2 instance profile if Terraform runs in AWS – which we likely will support by default as the underlying AWS SDK does so. Similarly for Azure (managed identity) and GCP (if running on GCP).
We might also consider features like assuming roles or using Azure Managed Service Identity via annotations if needed to deploy in certain constrained environments.
Another security consideration: storing sensitive data in state (like generated passwords). We will document best practices (e.g., if a password is output, to mark it sensitive or use Vault integration). Possibly in future, integrate with Vault or other secret stores to avoid cleartext secrets in state (beyond Terraform's own mechanisms).
Community Feedback and Flexibility: Finally, being an abstraction layer, we anticipate that not all use cases can be covered. The provider should make it easy to combine with regular provider resources if needed. For example, if a user needs an advanced S3 bucket feature not supported by abstract_bucket, they could create that bucket with the AWS provider separately and perhaps feed its name into other abstract resources. Or vice versa, someone might create a base infrastructure with abstract provider and then tweak one cloud’s resource with cloud-specific provider if necessary. Our design doesn’t prevent using the official AWS/Azure/GCP providers alongside – they just would manage separate resources. We will encourage feedback and perhaps implement toggles to relax or change behavior when it makes sense. For instance, if many users request the ability to have one Azure storage account hold multiple buckets to reduce cost, we could allow an annotation to specify an existing storage account id (and not delete it on destroy if not created by us). These kind of enhancements can be layered without affecting those who don’t use them.
Conclusion
The Abstract Cloud Provider for Terraform offers a unified, cloud-agnostic way to define core infrastructure primitives that span AWS, Azure, and Google Cloud. By embracing a lowest-common-denominator philosophy
developer.hashicorp.com
 and carefully mapping to each cloud’s services, it enables a dramatic simplification for organizations aiming for multi-cloud deployments. The design balances abstraction with flexibility: common infrastructure patterns are easy to deploy, while cloud-specific nuances can still be accessed via annotations when absolutely needed. Through clearly defined schemas for Cluster, Instance, Bucket, Function, Database, and Network resources, this provider covers the foundational pieces for cloud-agnostic application architecture. We illustrated how these resources correspond to each cloud’s offerings and provided examples of usage, including a hybrid scenario spanning all three providers. Implementation in Go
hashicorp.com
 ensures we can leverage Terraform’s proven provider SDK and each cloud’s official libraries for robust operation. The design handles multi-cloud interpolation naturally, letting Terraform orchestrate dependencies across disparate platforms in one plan/apply cycle. By focusing on portable services, we avoid the pitfalls warned by industry experts about trying to abstract everything
discuss.hashicorp.com
reddit.com
. Instead, we provide a pragmatic set of capabilities sufficient for many common workloads (web services, data processing pipelines, etc.) that need the flexibility of multi-cloud or the simplicity of a single config that can target any cloud environment. Moving forward, the provider can grow to support more resources and even additional clouds, guided always by the principle of consistent abstraction – only expanding where a truly analogous capability exists across platforms. This design document will serve as a blueprint for the implementation phase, ensuring that developers (and tools like Codex) have a clear specification to follow for building out the provider’s functionality. With this foundation, the Abstract Cloud Provider will empower users to write once and deploy anywhere, fulfilling the promise of cloud-agnostic infrastructure as code. Sources:
HashiCorp Terraform documentation on multi-cloud abstractions and the lowest common denominator approach
developer.hashicorp.com
.
Community discussions on cloud-agnostic Terraform design, highlighting the feasibility of abstracting services like Kubernetes clusters
discuss.hashicorp.com
 and the challenges of overly generic solutions
discuss.hashicorp.com
.
HashiCorp blog on Terraform providers indicating Go as the primary implementation language
hashicorp.com
.
Citations
Favicon
Module Composition | Terraform | HashiCorp Developer

https://developer.hashicorp.com/terraform/language/modules/develop/composition
Favicon
Writing Custom Terraform Providers

https://www.hashicorp.com/en/blog/writing-custom-terraform-providers
Favicon
Terraform in a cloud agnostic way - Terraform - HashiCorp Discuss

https://discuss.hashicorp.com/t/terraform-in-a-cloud-agnostic-way/45084
Favicon
How to write terraform in a provider independent way? : r/devops

https://www.reddit.com/r/devops/comments/lhkj1y/how_to_write_terraform_in_a_provider_independent/
Favicon
Terraform in a cloud agnostic way - Terraform - HashiCorp Discuss

https://discuss.hashicorp.com/t/terraform-in-a-cloud-agnostic-way/45084
Favicon
Terraform in a cloud agnostic way - Terraform - HashiCorp Discuss

https://discuss.hashicorp.com/t/terraform-in-a-cloud-agnostic-way/45084
Favicon
Module Composition | Terraform | HashiCorp Developer

https://developer.hashicorp.com/terraform/language/modules/develop/composition
All Sources
Favicon
developer.hashicorp
Favicon
hashicorp
Favicon
discuss.hashicorp
Favicon
reddit
